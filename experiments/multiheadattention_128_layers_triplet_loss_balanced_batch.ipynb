{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Triplets with majority class downsampled\n",
    "\n",
    "Siamese triplet loss training creates embedding spaces where similar items are pulled closer to one another, and dissimilar items are pushed away from one another. Siamese networks were independently introduced by both Bromley et al.(1993) and Baldi and Chauvin (1993) as a similarity-learning algorithm for signature verification and fingerprint verification, respectively. \n",
    "\n",
    "Instead of predicting a class label, these networks directly measure the similarity between samples of the same and differing classes. This is useful for scenarios where the number of classes is very large or unknownduring training, or where there is a only a few training samples per class(Chopraet al., 2005).\n",
    "\n",
    "For the sampling of triplets, we employ a technique called online semi-hard mining (Schroffet al., 2015). For a given minibatch, we first compute the embeddings for all the samples in the minibatch. To make up the triplets for the minibatch, all the possible positive anchor pairs $(\\boldsymbol{x}_a, \\boldsymbol{x}_p)$ are selected, and accompanied with a semi-hard negative that satisfies $D(\\boldsymbol{x}_a, \\boldsymbol{x}_p) < D(\\boldsymbol{x}_a, \\boldsymbol{x}_n) < D(\\boldsymbol{x}_a, \\boldsymbol{x}_p) + m$, where $D(\\cdot)$ is the distance function and $m$ is the margin. \n",
    "\n",
    "Further, we downsample the majority class (which makes up about 22% of the training set) to allow the model to learn more from the minority classes. \n",
    "\n",
    "We train the multi-head attention encoder architecture using siamese triplet loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.append(os.path.join(\\\"..\\\")) # path to source relative to current directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, TimeDistributed, Input, Flatten, AdditiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess_data\n",
    "import losses\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_7B', delimiter = ';', engine = 'python')\n",
    "data_text = data.loc[data['set'] == 'Train'][['helpdesk_question']]\n",
    "number_of_classes = data.loc[data['set'] == 'Train']['helpdesk_reply'].value_counts().shape[0]\n",
    "data = data[['helpdesk_question', 'helpdesk_reply', 'set', 'low_resource']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = pd.DataFrame(data.loc[data['set'] == 'Train']['helpdesk_reply'].value_counts()).reset_index()\n",
    "responses['reply'] = responses['index']\n",
    "responses['index'] = responses.index\n",
    "responses = dict(responses.set_index('reply')['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = preprocess_data.create_dictionary(data_text, 1, 0.25, 95000) #our entire vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.loc[data['set'] == 'Train']\n",
    "df_train = df_train.reset_index()[['helpdesk_question', 'helpdesk_reply']]\n",
    "df_train_keep = df_train\n",
    "#df_train = df_train.drop_duplicates()\n",
    "\n",
    "df_valid = data.loc[data['set'] == 'Valid']\n",
    "df_valid = df_valid.reset_index()[['helpdesk_question', 'helpdesk_reply']]\n",
    "\n",
    "df_test = data.loc[data['set'] == 'Test']\n",
    "df_test = df_test.reset_index()[['helpdesk_question', 'helpdesk_reply']]\n",
    "\n",
    "df_LR = data.loc[(data['set'] == 'Test') & (data['low_resource'] == 'True') ]\n",
    "df_LR = df_LR.reset_index()[['helpdesk_question', 'helpdesk_reply']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96412, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57545"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 30\n",
    "min_token_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = preprocess_data.create_lookup_tables(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the input sentence into a sequence of word IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96412, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x_word_ids = []\n",
    "for question in df_train['helpdesk_question'].apply(preprocess_data.preprocess_question, \n",
    "                                                    args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    train_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "train_x_word_ids = np.stack(train_x_word_ids)\n",
    "print(train_x_word_ids.shape)\n",
    "    \n",
    "val_x_word_ids = []\n",
    "for question in data['helpdesk_question'].loc[data['set'] == 'Valid'].apply(preprocess_data.preprocess_question, \n",
    "                                                                          args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    val_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "val_x_word_ids = np.stack(val_x_word_ids)\n",
    "\n",
    "test_x_word_ids = []\n",
    "for question in data['helpdesk_question'].loc[data['set'] == 'Test'].apply(preprocess_data.preprocess_question, \n",
    "                                                                          args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    test_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "    \n",
    "test_x_word_ids = np.stack(test_x_word_ids)\n",
    "\n",
    "LR_x_word_ids = []\n",
    "for question in data['helpdesk_question'].loc[(data['set'] == 'Test') & \n",
    "                                              (data['low_resource'] == 'True')].apply(preprocess_data.preprocess_question, \n",
    "                                                                          args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    LR_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "LR_x_word_ids = np.stack(LR_x_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(reply, all_responses):\n",
    "    \n",
    "    \"\"\" Constructs a one-hot vector for replies\n",
    "    \n",
    "    Args:\n",
    "        reply: query item \n",
    "        all_responses: dict containing all the template responses with their corresponding IDs\n",
    "    \n",
    "    Return:\n",
    "        a one-hot vector where the corresponding ID of the reply is the one-hot index\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Y = np.zeros(len(all_responses), dtype = int)\n",
    "    Y[all_responses[reply]] += 1\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_id(reply, all_responses):\n",
    "    \n",
    "    \"\"\" Returns integer ID corresponding to response for easy comparison and classification\n",
    "    \n",
    "    Args:\n",
    "        reply: query item \n",
    "        all_responses: dict containing all the template responses with their corresponding IDs\n",
    "        \n",
    "    Return: \n",
    "        integer corresponding to each response     \n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    return all_responses[reply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(list(df_train['helpdesk_reply'].apply(get_dummies, args = [responses])))\n",
    "valid_y = np.array(list(df_valid['helpdesk_reply'].apply(get_dummies, args = [responses])))\n",
    "test_y  = np.array(list(df_test['helpdesk_reply'].apply(get_dummies,  args = [responses])))\n",
    "LR_y    = np.array(list(df_LR['helpdesk_reply'].apply(get_dummies,    args = [responses])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_word_ids = train_x_word_ids.reshape(train_x_word_ids.shape[:-1])\n",
    "val_x_word_ids   = val_x_word_ids.reshape(val_x_word_ids.shape[:-1])\n",
    "test_x_word_ids  = test_x_word_ids.reshape(test_x_word_ids.shape[:-1])\n",
    "LR_x_word_ids    = LR_x_word_ids.reshape(LR_x_word_ids.shape[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove vectors where the input sentence yields a sequence of length 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zero_vectors = np.where(train_x_word_ids.sum(axis = 1) == 0.0)[0]\n",
    "for t in range(train_zero_vectors.shape[0]):\n",
    "    train_x_word_ids[train_zero_vectors[t]][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_zero_vectors = np.where(val_x_word_ids.sum(axis = 1) == 0.0)[0]\n",
    "for t in range(val_zero_vectors.shape[0]):\n",
    "    val_x_word_ids[val_zero_vectors[t]][0] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the encoder (from the Transformer)\n",
    "\n",
    "Original code obtained from https://www.tensorflow.org/tutorials/text/transformer with minor adaptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    \n",
    "    \"\"\" Multiplying angle rates and positions gives a map of the position encoding angles as a \n",
    "    function of depth. The angle rates range from 1 [rads/step] to min_rate [rads/step] over the \n",
    "    vector depth.\n",
    "    \n",
    "    Args:\n",
    "        pos: vector of positions\n",
    "        i: embedding vector\n",
    "        d_model: dimension of embedding vector\n",
    "        \n",
    "    Returns:\n",
    "        Vector of angle radians\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    angle_rate = 1/np.power(10000, ((2*i)/np.float32(d_model)))\n",
    "    return pos * angle_rate\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \n",
    "    \"\"\" Calculate positional encodings to inject information about relative and absolute positions/\n",
    "    The positional encodings are obtained by taking the sine and cosine of the angle radians.\n",
    "    \n",
    "    Args:\n",
    "        position: maximum position encoding\n",
    "        d_model: dimension of embedding vector\n",
    "    \n",
    "    Returns:\n",
    "        A positional encoding vector\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], \n",
    "                            np.arange(d_model)[np.newaxis, :], \n",
    "                            d_model)\n",
    "    \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \n",
    "    \"\"\" Calculate the attention weights. q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\" Multi-head attention consists of four parts: linear layers that split into heads, \n",
    "    scaled dot-product attention, the concatenation of heads, and a final linear layer.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \n",
    "        \"\"\" Split the last dimension into (num_heads, depth). \n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        Args:\n",
    "            x: feed forward layer\n",
    "            batch_size: number of items in a batch\n",
    "            \n",
    "        Returns:\n",
    "            tuple containing (batch size, number of heads, sequence length, depth)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        \n",
    "        \"\"\" Call function to split the heads of the linear layers. \n",
    "        Returns the scaled attention dense layer and attention weights\n",
    "        \n",
    "        Args:\n",
    "            q: query shape == (..., seq_len_q, depth)\n",
    "            k: key shape == (..., seq_len_k, depth)\n",
    "            v: value shape == (..., seq_len_v, depth_v)\n",
    "            mask: float tensor with shape broadcastable \n",
    "            \n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, \n",
    "                                                                              #seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    \n",
    "    \"\"\" Construct a two-layer feedforward NN with layer dimensions d_model and dff respectively \n",
    "    and ReLU activations between layers.\n",
    "    \n",
    "    Args:\n",
    "        d_model: dimension of embedding layer\n",
    "        dff: dimension of the second layer\n",
    "    \n",
    "    Returns:\n",
    "        A two-layer feedforward NN \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\" Each encoder layer consists of Multi-head attention (with padding mask) and pointwise \n",
    "    feedforward networks.\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        \n",
    "        \"\"\" Constructs the encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            x: sequential layer\n",
    "            training: flag indicating training or testing\n",
    "            mask: float tensor with shape broadcastable \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\" The Encoder consists of an input embedding, summed with positional encoding, and N encoder layers. \n",
    "    The summation is the input to the encoder layers. The output of the encoder is the input to the decoder.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers       \n",
    "        self.embedding = Embedding(input_vocab_size, d_model,)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask=None):\n",
    "        \n",
    "        \"\"\" This function constructs the encoder.\n",
    "        Note we move the dropout to right before the summation (of embedding and positional encodings).\n",
    "        \n",
    "        Args: \n",
    "            x: sequential layer\n",
    "            training: flag indicating training or testing\n",
    "            mask: float tensor with shape broadcastable \n",
    "            \n",
    "        Returns:\n",
    "            An encoder model \n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]        \n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x, training = training)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        #x = self.dropout(x, training = training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention_encoder(num_layers, max_features, input_length=30, model_dim=512, dff = 128, \n",
    "                                num_heads=4):\n",
    "    \n",
    "    \"\"\" Constructs a multihead attention encoder model\n",
    "    \n",
    "    Args:\n",
    "        num_layers: number of encoder layers\n",
    "        max_features: size of vocabulary\n",
    "        input_length: length of input sequence\n",
    "        model_dim: dimension of embedding vector\n",
    "        dff: dimension of second layer in pointwise FFNN\n",
    "        num_heads: number of heads to split\n",
    "    \n",
    "    Returns:\n",
    "        Model object\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = Input(shape=(input_length, ))\n",
    "    x = Encoder(num_layers, model_dim, num_heads, dff, max_features, maximum_position_encoding = 10000, \n",
    "                rate=0.5)(inputs)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(300, activation=None)(x)\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Attention Encoder with Average Pooling\n",
    "\n",
    "We use average pooling to construct a single feature vector from the variable-length sequence of encodings produced by the MHA Encoder. This is then connected to a single dense layer with 300 dimensions. Our MHA has 8 heads, 2 layers, and dropout of 50% to regularize the model during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_features = len(unique_words) + 1\n",
    "num_layers = 2\n",
    "\n",
    "model = multihead_attention_encoder(num_layers, max_features, input_length=30, model_dim=128,\n",
    "                                    num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, 30, 128)           7564928   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 300)               38700     \n",
      "=================================================================\n",
      "Total params: 7,603,628\n",
      "Trainable params: 7,603,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Triplet Loss Training\n",
    "\n",
    "We perform the Siamese triplet loss training with mini-batch sizes of 256, cosine as our distance function and a margin $m$ of 0.5. For online sampling we use a batch size of 256. Larger batch sizes consumed too much memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.triplet_semihard_loss(margin=0.5, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', verbose=1, restore_best_weights=True, patience=50)\n",
    "model.compile(loss=loss, optimizer=tf.keras.optimizers.Adadelta(learning_rate= 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Batches\n",
    "\n",
    "Create balanced batches by downsampling majority class (which makes up 22% of the training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.keras import BalancedBatchGenerator\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = BalancedBatchGenerator(train_x_word_ids, \n",
    "                                            np.array(df_train['helpdesk_reply'].apply(get_label_id, \n",
    "                                                                                      args = [responses])),\n",
    "                                            sampler = RandomUnderSampler(sampling_strategy='majority'),                                        \n",
    "                                            batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "360/360 [==============================] - 40s 112ms/step - loss: 0.4997 - val_loss: 0.4990\n",
      "Epoch 2/1000\n",
      "360/360 [==============================] - 51s 141ms/step - loss: 0.4988 - val_loss: 0.4960\n",
      "Epoch 3/1000\n",
      "360/360 [==============================] - 50s 139ms/step - loss: 0.4974 - val_loss: 0.4938\n",
      "Epoch 4/1000\n",
      "360/360 [==============================] - 50s 138ms/step - loss: 0.4951 - val_loss: 0.4901\n",
      "Epoch 5/1000\n",
      "360/360 [==============================] - 50s 138ms/step - loss: 0.4879 - val_loss: 0.4850\n",
      "Epoch 6/1000\n",
      "360/360 [==============================] - 50s 139ms/step - loss: 0.4839 - val_loss: 0.4837\n",
      "Epoch 7/1000\n",
      "360/360 [==============================] - 49s 137ms/step - loss: 0.4808 - val_loss: 0.4819\n",
      "Epoch 8/1000\n",
      "360/360 [==============================] - 49s 137ms/step - loss: 0.4803 - val_loss: 0.4825\n",
      "Epoch 9/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4772 - val_loss: 0.4805\n",
      "Epoch 10/1000\n",
      "360/360 [==============================] - 47s 131ms/step - loss: 0.4757 - val_loss: 0.4798\n",
      "Epoch 11/1000\n",
      "360/360 [==============================] - 49s 137ms/step - loss: 0.4733 - val_loss: 0.4796\n",
      "Epoch 12/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4714 - val_loss: 0.4771\n",
      "Epoch 13/1000\n",
      "360/360 [==============================] - 50s 138ms/step - loss: 0.4692 - val_loss: 0.4764\n",
      "Epoch 14/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4670 - val_loss: 0.4763\n",
      "Epoch 15/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4636 - val_loss: 0.4742\n",
      "Epoch 16/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4627 - val_loss: 0.4724\n",
      "Epoch 17/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4597 - val_loss: 0.4733\n",
      "Epoch 18/1000\n",
      "360/360 [==============================] - 48s 135ms/step - loss: 0.4587 - val_loss: 0.4702\n",
      "Epoch 19/1000\n",
      "360/360 [==============================] - 49s 135ms/step - loss: 0.4561 - val_loss: 0.4702\n",
      "Epoch 20/1000\n",
      "360/360 [==============================] - 50s 139ms/step - loss: 0.4550 - val_loss: 0.4702\n",
      "Epoch 21/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4535 - val_loss: 0.4699\n",
      "Epoch 22/1000\n",
      "360/360 [==============================] - 49s 135ms/step - loss: 0.4525 - val_loss: 0.4691\n",
      "Epoch 23/1000\n",
      "360/360 [==============================] - 49s 137ms/step - loss: 0.4485 - val_loss: 0.4687\n",
      "Epoch 24/1000\n",
      "360/360 [==============================] - 50s 138ms/step - loss: 0.4493 - val_loss: 0.4659\n",
      "Epoch 25/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4466 - val_loss: 0.4674\n",
      "Epoch 26/1000\n",
      "360/360 [==============================] - 49s 137ms/step - loss: 0.4456 - val_loss: 0.4669\n",
      "Epoch 27/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4454 - val_loss: 0.4675\n",
      "Epoch 28/1000\n",
      "360/360 [==============================] - 47s 132ms/step - loss: 0.4437 - val_loss: 0.4651\n",
      "Epoch 29/1000\n",
      "360/360 [==============================] - 48s 133ms/step - loss: 0.4433 - val_loss: 0.4653\n",
      "Epoch 30/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4408 - val_loss: 0.4644\n",
      "Epoch 31/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4408 - val_loss: 0.4651\n",
      "Epoch 32/1000\n",
      "360/360 [==============================] - 48s 135ms/step - loss: 0.4386 - val_loss: 0.4664\n",
      "Epoch 33/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4379 - val_loss: 0.4636\n",
      "Epoch 34/1000\n",
      "360/360 [==============================] - 49s 135ms/step - loss: 0.4364 - val_loss: 0.4637\n",
      "Epoch 35/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4356 - val_loss: 0.4623\n",
      "Epoch 36/1000\n",
      "360/360 [==============================] - 49s 135ms/step - loss: 0.4353 - val_loss: 0.4648\n",
      "Epoch 37/1000\n",
      "360/360 [==============================] - 49s 135ms/step - loss: 0.4322 - val_loss: 0.4635\n",
      "Epoch 38/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4339 - val_loss: 0.4630\n",
      "Epoch 39/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4295 - val_loss: 0.4623\n",
      "Epoch 40/1000\n",
      "360/360 [==============================] - 49s 135ms/step - loss: 0.4328 - val_loss: 0.4637\n",
      "Epoch 41/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4292 - val_loss: 0.4609\n",
      "Epoch 42/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4296 - val_loss: 0.4615\n",
      "Epoch 43/1000\n",
      "360/360 [==============================] - 49s 137ms/step - loss: 0.4284 - val_loss: 0.4603\n",
      "Epoch 44/1000\n",
      "360/360 [==============================] - 48s 132ms/step - loss: 0.4248 - val_loss: 0.4612\n",
      "Epoch 45/1000\n",
      "360/360 [==============================] - 50s 139ms/step - loss: 0.4273 - val_loss: 0.4610\n",
      "Epoch 46/1000\n",
      "360/360 [==============================] - 49s 135ms/step - loss: 0.4276 - val_loss: 0.4608\n",
      "Epoch 47/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4234 - val_loss: 0.4612\n",
      "Epoch 48/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4243 - val_loss: 0.4615\n",
      "Epoch 49/1000\n",
      "360/360 [==============================] - 50s 138ms/step - loss: 0.4213 - val_loss: 0.4591\n",
      "Epoch 50/1000\n",
      "360/360 [==============================] - 49s 135ms/step - loss: 0.4208 - val_loss: 0.4600\n",
      "Epoch 51/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4205 - val_loss: 0.4584\n",
      "Epoch 52/1000\n",
      "360/360 [==============================] - 50s 138ms/step - loss: 0.4214 - val_loss: 0.4597\n",
      "Epoch 53/1000\n",
      "360/360 [==============================] - 48s 134ms/step - loss: 0.4164 - val_loss: 0.4595\n",
      "Epoch 54/1000\n",
      "360/360 [==============================] - 48s 133ms/step - loss: 0.4208 - val_loss: 0.4605\n",
      "Epoch 55/1000\n",
      "360/360 [==============================] - 49s 136ms/step - loss: 0.4192 - val_loss: 0.4593\n",
      "Epoch 56/1000\n",
      "360/360 [==============================] - 28s 78ms/step - loss: 0.4145 - val_loss: 0.4600\n",
      "Epoch 57/1000\n",
      "360/360 [==============================] - 27s 74ms/step - loss: 0.4153 - val_loss: 0.4592\n",
      "Epoch 58/1000\n",
      "360/360 [==============================] - 32s 89ms/step - loss: 0.4143 - val_loss: 0.4609\n",
      "Epoch 59/1000\n",
      "360/360 [==============================] - 31s 85ms/step - loss: 0.4150 - val_loss: 0.4594\n",
      "Epoch 60/1000\n",
      "360/360 [==============================] - 27s 74ms/step - loss: 0.4151 - val_loss: 0.4593\n",
      "Epoch 61/1000\n",
      "360/360 [==============================] - 27s 74ms/step - loss: 0.4134 - val_loss: 0.4589\n",
      "Epoch 62/1000\n",
      "360/360 [==============================] - 27s 74ms/step - loss: 0.4131 - val_loss: 0.4587\n",
      "Epoch 63/1000\n",
      "360/360 [==============================] - 29s 81ms/step - loss: 0.4096 - val_loss: 0.4585\n",
      "Epoch 64/1000\n",
      "360/360 [==============================] - 27s 74ms/step - loss: 0.4108 - val_loss: 0.4589\n",
      "Epoch 65/1000\n",
      "360/360 [==============================] - 27s 75ms/step - loss: 0.4116 - val_loss: 0.4576\n",
      "Epoch 66/1000\n",
      "360/360 [==============================] - 27s 75ms/step - loss: 0.4103 - val_loss: 0.4574\n",
      "Epoch 67/1000\n",
      "360/360 [==============================] - 27s 74ms/step - loss: 0.4098 - val_loss: 0.4576\n",
      "Epoch 68/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.4096 - val_loss: 0.4573\n",
      "Epoch 69/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.4092 - val_loss: 0.4579\n",
      "Epoch 70/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.4056 - val_loss: 0.4581\n",
      "Epoch 71/1000\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.4090 - val_loss: 0.4572\n",
      "Epoch 72/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.4061 - val_loss: 0.4576\n",
      "Epoch 73/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.4061 - val_loss: 0.4579\n",
      "Epoch 74/1000\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.4052 - val_loss: 0.4576\n",
      "Epoch 75/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.4029 - val_loss: 0.4581\n",
      "Epoch 76/1000\n",
      "360/360 [==============================] - 24s 66ms/step - loss: 0.4055 - val_loss: 0.4581\n",
      "Epoch 77/1000\n",
      "360/360 [==============================] - 24s 66ms/step - loss: 0.4018 - val_loss: 0.4575\n",
      "Epoch 78/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 23s 64ms/step - loss: 0.4006 - val_loss: 0.4570\n",
      "Epoch 79/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.4021 - val_loss: 0.4588\n",
      "Epoch 80/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.4007 - val_loss: 0.4577\n",
      "Epoch 81/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3998 - val_loss: 0.4552\n",
      "Epoch 82/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3997 - val_loss: 0.4576\n",
      "Epoch 83/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3984 - val_loss: 0.4573\n",
      "Epoch 84/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3984 - val_loss: 0.4578\n",
      "Epoch 85/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3999 - val_loss: 0.4566\n",
      "Epoch 86/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3979 - val_loss: 0.4567\n",
      "Epoch 87/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3959 - val_loss: 0.4578\n",
      "Epoch 88/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.4012 - val_loss: 0.4575\n",
      "Epoch 89/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3962 - val_loss: 0.4563\n",
      "Epoch 90/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3944 - val_loss: 0.4584\n",
      "Epoch 91/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3926 - val_loss: 0.4561\n",
      "Epoch 92/1000\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.3959 - val_loss: 0.4572\n",
      "Epoch 93/1000\n",
      "360/360 [==============================] - 24s 66ms/step - loss: 0.3952 - val_loss: 0.4575\n",
      "Epoch 94/1000\n",
      "360/360 [==============================] - 24s 65ms/step - loss: 0.3941 - val_loss: 0.4577\n",
      "Epoch 95/1000\n",
      "360/360 [==============================] - 24s 66ms/step - loss: 0.3909 - val_loss: 0.4568\n",
      "Epoch 96/1000\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.3922 - val_loss: 0.4576\n",
      "Epoch 97/1000\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.3938 - val_loss: 0.4568\n",
      "Epoch 98/1000\n",
      "360/360 [==============================] - 26s 71ms/step - loss: 0.3923 - val_loss: 0.4570\n",
      "Epoch 99/1000\n",
      "360/360 [==============================] - 40s 111ms/step - loss: 0.3923 - val_loss: 0.4558\n",
      "Epoch 100/1000\n",
      "360/360 [==============================] - 40s 112ms/step - loss: 0.3909 - val_loss: 0.4570\n",
      "Epoch 101/1000\n",
      "360/360 [==============================] - 38s 106ms/step - loss: 0.3918 - val_loss: 0.4574\n",
      "Epoch 102/1000\n",
      "360/360 [==============================] - 33s 92ms/step - loss: 0.3896 - val_loss: 0.4558\n",
      "Epoch 103/1000\n",
      "360/360 [==============================] - 33s 91ms/step - loss: 0.3900 - val_loss: 0.4571\n",
      "Epoch 104/1000\n",
      "360/360 [==============================] - 34s 93ms/step - loss: 0.3875 - val_loss: 0.4566\n",
      "Epoch 105/1000\n",
      "360/360 [==============================] - 37s 103ms/step - loss: 0.3900 - val_loss: 0.4583\n",
      "Epoch 106/1000\n",
      "360/360 [==============================] - 32s 88ms/step - loss: 0.3906 - val_loss: 0.4569\n",
      "Epoch 107/1000\n",
      "360/360 [==============================] - 33s 92ms/step - loss: 0.3887 - val_loss: 0.4582\n",
      "Epoch 108/1000\n",
      "360/360 [==============================] - 33s 90ms/step - loss: 0.3883 - val_loss: 0.4574\n",
      "Epoch 109/1000\n",
      "360/360 [==============================] - 31s 87ms/step - loss: 0.3879 - val_loss: 0.4558\n",
      "Epoch 110/1000\n",
      "360/360 [==============================] - 24s 66ms/step - loss: 0.3854 - val_loss: 0.4569\n",
      "Epoch 111/1000\n",
      "360/360 [==============================] - 24s 66ms/step - loss: 0.3878 - val_loss: 0.4560\n",
      "Epoch 112/1000\n",
      "360/360 [==============================] - 24s 66ms/step - loss: 0.3871 - val_loss: 0.4556\n",
      "Epoch 113/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3883 - val_loss: 0.4564\n",
      "Epoch 114/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3885 - val_loss: 0.4563\n",
      "Epoch 115/1000\n",
      "360/360 [==============================] - 28s 79ms/step - loss: 0.3801 - val_loss: 0.4570\n",
      "Epoch 116/1000\n",
      "360/360 [==============================] - 35s 97ms/step - loss: 0.3823 - val_loss: 0.4556\n",
      "Epoch 117/1000\n",
      "360/360 [==============================] - 33s 92ms/step - loss: 0.3815 - val_loss: 0.4571\n",
      "Epoch 118/1000\n",
      "360/360 [==============================] - 29s 79ms/step - loss: 0.3831 - val_loss: 0.4579\n",
      "Epoch 119/1000\n",
      "360/360 [==============================] - 28s 79ms/step - loss: 0.3847 - val_loss: 0.4569\n",
      "Epoch 120/1000\n",
      "360/360 [==============================] - 28s 79ms/step - loss: 0.3826 - val_loss: 0.4563\n",
      "Epoch 121/1000\n",
      "360/360 [==============================] - 32s 88ms/step - loss: 0.3814 - val_loss: 0.4566\n",
      "Epoch 122/1000\n",
      "360/360 [==============================] - 28s 79ms/step - loss: 0.3809 - val_loss: 0.4560\n",
      "Epoch 123/1000\n",
      "360/360 [==============================] - 28s 79ms/step - loss: 0.3831 - val_loss: 0.4547\n",
      "Epoch 124/1000\n",
      "360/360 [==============================] - 29s 81ms/step - loss: 0.3805 - val_loss: 0.4572\n",
      "Epoch 125/1000\n",
      "360/360 [==============================] - 26s 72ms/step - loss: 0.3827 - val_loss: 0.4573\n",
      "Epoch 126/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3825 - val_loss: 0.4574\n",
      "Epoch 127/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3840 - val_loss: 0.4568\n",
      "Epoch 128/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3800 - val_loss: 0.4569\n",
      "Epoch 129/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3802 - val_loss: 0.4567\n",
      "Epoch 130/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3793 - val_loss: 0.4555\n",
      "Epoch 131/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3785 - val_loss: 0.4548\n",
      "Epoch 132/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3791 - val_loss: 0.4562\n",
      "Epoch 133/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3766 - val_loss: 0.4565\n",
      "Epoch 134/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3784 - val_loss: 0.4576\n",
      "Epoch 135/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3769 - val_loss: 0.4555\n",
      "Epoch 136/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3788 - val_loss: 0.4567\n",
      "Epoch 137/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3751 - val_loss: 0.4558\n",
      "Epoch 138/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3789 - val_loss: 0.4560\n",
      "Epoch 139/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3739 - val_loss: 0.4564\n",
      "Epoch 140/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3754 - val_loss: 0.4571\n",
      "Epoch 141/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3740 - val_loss: 0.4576\n",
      "Epoch 142/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3759 - val_loss: 0.4569\n",
      "Epoch 143/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3760 - val_loss: 0.4554\n",
      "Epoch 144/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3750 - val_loss: 0.4557\n",
      "Epoch 145/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3734 - val_loss: 0.4535\n",
      "Epoch 146/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3759 - val_loss: 0.4572\n",
      "Epoch 147/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3759 - val_loss: 0.4564\n",
      "Epoch 148/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3755 - val_loss: 0.4603\n",
      "Epoch 149/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3727 - val_loss: 0.4545\n",
      "Epoch 150/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3753 - val_loss: 0.4555\n",
      "Epoch 151/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3746 - val_loss: 0.4545\n",
      "Epoch 152/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3746 - val_loss: 0.4567\n",
      "Epoch 153/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3725 - val_loss: 0.4543\n",
      "Epoch 154/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3721 - val_loss: 0.4565\n",
      "Epoch 155/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3722 - val_loss: 0.4553\n",
      "Epoch 156/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3720 - val_loss: 0.4567\n",
      "Epoch 157/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3704 - val_loss: 0.4581\n",
      "Epoch 158/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3734 - val_loss: 0.4541\n",
      "Epoch 159/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3709 - val_loss: 0.4540\n",
      "Epoch 160/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3709 - val_loss: 0.4554\n",
      "Epoch 161/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3687 - val_loss: 0.4566\n",
      "Epoch 162/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3693 - val_loss: 0.4560\n",
      "Epoch 163/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3657 - val_loss: 0.4547\n",
      "Epoch 164/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3678 - val_loss: 0.4539\n",
      "Epoch 165/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3690 - val_loss: 0.4563\n",
      "Epoch 166/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3680 - val_loss: 0.4569\n",
      "Epoch 167/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3717 - val_loss: 0.4548\n",
      "Epoch 168/1000\n",
      "360/360 [==============================] - 22s 62ms/step - loss: 0.3661 - val_loss: 0.4550\n",
      "Epoch 169/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3656 - val_loss: 0.4543\n",
      "Epoch 170/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3682 - val_loss: 0.4554\n",
      "Epoch 171/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3666 - val_loss: 0.4558\n",
      "Epoch 172/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3664 - val_loss: 0.4553\n",
      "Epoch 173/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3673 - val_loss: 0.4561\n",
      "Epoch 174/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3682 - val_loss: 0.4598\n",
      "Epoch 175/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3678 - val_loss: 0.4544\n",
      "Epoch 176/1000\n",
      "360/360 [==============================] - 22s 62ms/step - loss: 0.3650 - val_loss: 0.4554\n",
      "Epoch 177/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3648 - val_loss: 0.4564\n",
      "Epoch 178/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3663 - val_loss: 0.4559\n",
      "Epoch 179/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3646 - val_loss: 0.4551\n",
      "Epoch 180/1000\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.3635 - val_loss: 0.4569\n",
      "Epoch 181/1000\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.3680 - val_loss: 0.4559\n",
      "Epoch 182/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3648 - val_loss: 0.4549\n",
      "Epoch 183/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3619 - val_loss: 0.4550\n",
      "Epoch 184/1000\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.3639 - val_loss: 0.4557\n",
      "Epoch 185/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3669 - val_loss: 0.4563\n",
      "Epoch 186/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3630 - val_loss: 0.4548\n",
      "Epoch 187/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3634 - val_loss: 0.4546\n",
      "Epoch 188/1000\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.3650 - val_loss: 0.4546\n",
      "Epoch 189/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3632 - val_loss: 0.4563\n",
      "Epoch 190/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3648 - val_loss: 0.4550\n",
      "Epoch 191/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3626 - val_loss: 0.4559\n",
      "Epoch 192/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3607 - val_loss: 0.4562\n",
      "Epoch 193/1000\n",
      "360/360 [==============================] - 23s 63ms/step - loss: 0.3572 - val_loss: 0.4573\n",
      "Epoch 194/1000\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3626 - val_loss: 0.4559\n",
      "Epoch 195/1000\n",
      "359/360 [============================>.] - ETA: 0s - loss: 0.3603Restoring model weights from the end of the best epoch.\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 0.3602 - val_loss: 0.4542\n",
      "Epoch 00195: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f74694d82e8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_generator, steps_per_epoch=360, epochs=1000,         \n",
    "          callbacks=[es],\n",
    "          validation_data=(val_x_word_ids, np.array(df_valid['helpdesk_reply'].apply(get_label_id, \n",
    "                                                                                     args = [responses]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_preprocess(entry):\n",
    "    if responses.get(entry) != None:\n",
    "        return responses[entry]\n",
    "    else:\n",
    "        return len(responses) #default unknown class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = model.predict(train_x_word_ids)\n",
    "y_train = df_train_keep['helpdesk_reply'].apply(label_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = model.predict(val_x_word_ids)\n",
    "y_valid = df_valid['helpdesk_reply'].apply(label_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = model.predict(test_x_word_ids)\n",
    "y_test = df_test['helpdesk_reply'].apply(label_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_LR = model.predict(LR_x_word_ids)\n",
    "y_LR = df_LR['helpdesk_reply'].apply(label_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn_model(x_train, y_train, metric, k, weights):\n",
    "    print(k, 'Nearest Neighbours')\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, weights= weights, metric = metric)\n",
    "    clf.fit(x_train, y_train)\n",
    "    #print(\"Train accuracy\", clf.score(x_train, y_train))\n",
    "        \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Nearest Neighbours\n",
      "Train accuracy 0.9678048375720865\n",
      "Validation accuracy 0.5196682835237052\n"
     ]
    }
   ],
   "source": [
    "clf_1NN = train_knn_model(x_train = x_train, y_train = y_train, metric = 'cosine', \n",
    "                          k = 1, weights = 'distance')\n",
    "score = clf_1NN.score(x_train, y_train)\n",
    "print(\"Train accuracy\", score)\n",
    "score = clf_1NN.score(x_valid, y_valid)\n",
    "print(\"Validation accuracy\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Nearest Neighbours\n",
      "Validation accuracy 0.5605695509309967\n"
     ]
    }
   ],
   "source": [
    "clf_5NN = train_knn_model(x_train = x_train, y_train = y_train, metric = 'cosine', \n",
    "                          k = 5, weights = 'distance')\n",
    "score = clf_5NN.score(x_valid, y_valid)\n",
    "print(\"Validation accuracy\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Nearest Neighbours\n",
      "Validation accuracy 0.5897355656391801\n"
     ]
    }
   ],
   "source": [
    "clf_25NN = train_knn_model(x_train = x_train, y_train = y_train, metric = 'cosine', \n",
    "                          k = 25, weights = 'distance')\n",
    "score = clf_25NN.score(x_valid, y_valid)\n",
    "print(\"Validation accuracy\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Nearest Neighbours\n",
      "Validation accuracy 0.591237677984666\n"
     ]
    }
   ],
   "source": [
    "clf_50NN = train_knn_model(x_train = x_train, y_train = y_train, metric = 'cosine', \n",
    "                          k = 50, weights = 'distance')\n",
    "score = clf_50NN.score(x_valid, y_valid)\n",
    "print(\"Validation accuracy\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on 1-NN 0.5270375081438278\n",
      "Test accuracy on 5-NN 0.5672757732758353\n",
      "Test accuracy on 25-NN 0.592870660503211\n",
      "Test accuracy on 50-NN 0.5944218657897186\n"
     ]
    }
   ],
   "source": [
    "score = clf_1NN.score(x_test, y_test)\n",
    "print(\"Test accuracy on 1-NN\", score)\n",
    "score = clf_5NN.score(x_test, y_test)\n",
    "print(\"Test accuracy on 5-NN\", score)\n",
    "score = clf_25NN.score(x_test, y_test)\n",
    "print(\"Test accuracy on 25-NN\", score)\n",
    "score = clf_50NN.score(x_test, y_test)\n",
    "print(\"Test accuracy on 50-NN\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Test accuracy on 1-NN 0.42367788461538464\n",
      "LR Test accuracy on 5-NN 0.4690504807692308\n",
      "LR Test accuracy on 25-NN 0.5120192307692307\n",
      "LR Test accuracy on 50-NN 0.5141225961538461\n"
     ]
    }
   ],
   "source": [
    "score = clf_1NN.score(x_LR, y_LR)\n",
    "print(\"LR Test accuracy on 1-NN\", score)\n",
    "score = clf_5NN.score(x_LR, y_LR)\n",
    "print(\"LR Test accuracy on 5-NN\", score)\n",
    "score = clf_25NN.score(x_LR, y_LR)\n",
    "print(\"LR Test accuracy on 25-NN\", score)\n",
    "score = clf_50NN.score(x_LR, y_LR)\n",
    "print(\"LR Test accuracy on 50-NN\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the quality of cross-lingual embeddings\n",
    "\n",
    "We design a small experiment to assess the quality of the cross-lingual embeddings for English and Zulu. The translations were obtained using google translate and verified by a Zulu speaker. We compute the sentence embedding for each English-Zulu translation pair and calculate the cosine distance between the two embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_A  = \"can you drink coca cola when you are pregnant\"\n",
    "zulu_A = \"ungayiphuza yini i-coca cola uma ukhulelwe\"\n",
    "\n",
    "eng_B  = \"when can i stop breastfeeding\"\n",
    "zulu_B = \"ngingakuyeka nini ukuncelisa ibele\"\n",
    "\n",
    "eng_C  = \"when can I start feeding my baby solid food\"\n",
    "zulu_C = \"ngingaqala nini ukondla ingane yami ukudla okuqinile\"\n",
    "\n",
    "eng_D  = \"what are the signs of labour\"\n",
    "zulu_D = \"yiziphi izimpawu zokubeletha\"\n",
    "\n",
    "eng_E  = \"when can I learn the gender of my baby\"\n",
    "zulu_E = \"ngingabazi ubulili bengane yami\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words['yami']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_embeddings(question, model, unique_words, min_token_length, word_to_id):\n",
    "    q = preprocess_data.preprocess_question(question, unique_words, min_token_length)\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(q, word_to_id)\n",
    "    word_ids = np.array(word_ids, dtype = float)\n",
    "    word_ids = word_ids.reshape((1, word_ids.shape[0]))\n",
    "    embedding = model.predict(word_ids)\n",
    "    return embedding    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_eng_A = create_sentence_embeddings(eng_A, model, unique_words, min_token_length, word_to_id)\n",
    "embed_eng_B = create_sentence_embeddings(eng_B, model, unique_words, min_token_length, word_to_id)\n",
    "embed_eng_C = create_sentence_embeddings(eng_C, model, unique_words, min_token_length, word_to_id)\n",
    "embed_eng_D = create_sentence_embeddings(eng_D, model, unique_words, min_token_length, word_to_id)\n",
    "embed_eng_E = create_sentence_embeddings(eng_E, model, unique_words, min_token_length, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_zulu_A = create_sentence_embeddings(zulu_A, model, unique_words, min_token_length, word_to_id)\n",
    "embed_zulu_B = create_sentence_embeddings(zulu_B, model, unique_words, min_token_length, word_to_id)\n",
    "embed_zulu_C = create_sentence_embeddings(zulu_C, model, unique_words, min_token_length, word_to_id)\n",
    "embed_zulu_D = create_sentence_embeddings(zulu_D, model, unique_words, min_token_length, word_to_id)\n",
    "embed_zulu_E = create_sentence_embeddings(zulu_E, model, unique_words, min_token_length, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence A: 0.36851388216018677\n",
      "Sentence B: 0.2721353769302368\n",
      "Sentence C: 0.1550511121749878\n",
      "Sentence D: 0.11843031644821167\n",
      "Sentence E: 0.9600929133594036\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence A:\", cosine(embed_eng_A, embed_zulu_A))\n",
    "print(\"Sentence B:\", cosine(embed_eng_B, embed_zulu_B))\n",
    "print(\"Sentence C:\", cosine(embed_eng_C, embed_zulu_C))\n",
    "print(\"Sentence D:\", cosine(embed_eng_D, embed_zulu_D))\n",
    "print(\"Sentence E:\", cosine(embed_eng_E, embed_zulu_E))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
