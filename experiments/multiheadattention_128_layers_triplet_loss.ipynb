{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Triplets \n",
    "\n",
    "Siamese triplet loss training creates embedding spaces where similar items are pulled closer to one another, and dissimilar items are pushed away from one another. Siamese networks were independently introduced by both Bromley et al.(1993) and Baldi and Chauvin (1993) as a similarity-learning algorithm for signature verification and fingerprint verification, respectively. \n",
    "\n",
    "Instead of predicting a class label, these networks directly measure the similarity between samples of the same and differing classes. This is useful for scenarios where the number of classes is very large or unknownduring training, or where there is a only a few training samples per class(Chopraet al., 2005).\n",
    "\n",
    "For the sampling of triplets, we employ a technique called online semi-hard mining (Schroffet al., 2015). For a given minibatch, we first compute the embeddings for all the samples in the minibatch. To make up the triplets for the minibatch, all the possible positive anchor pairs $(\\boldsymbol{x}_a, \\boldsymbol{x}_p)$ are selected, and accompanied with a semi-hard negative that satisfies $D(\\boldsymbol{x}_a, \\boldsymbol{x}_p) < D(\\boldsymbol{x}_a, \\boldsymbol{x}_n) < D(\\boldsymbol{x}_a, \\boldsymbol{x}_p) + m$, where $D(\\cdot)$ is the distance function and $m$ is the margin. \n",
    "\n",
    "We train the multi-head attention encoder architecture using siamese triplet loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.append(os.path.join(\\\"..\\\")) # path to source relative to current directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, TimeDistributed, Input, Flatten, AdditiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess_data\n",
    "import losses\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_7B', delimiter = ';', engine = 'python')\n",
    "data_text = data.loc[data['set'] == 'Train'][['helpdesk_question']]\n",
    "number_of_classes = data.loc[data['set'] == 'Train']['helpdesk_reply'].value_counts().shape[0]\n",
    "data = data[['helpdesk_question', 'helpdesk_reply', 'set', 'low_resource']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = pd.DataFrame(data.loc[data['set'] == 'Train']['helpdesk_reply'].value_counts()).reset_index()\n",
    "responses['reply'] = responses['index']\n",
    "responses['index'] = responses.index\n",
    "responses = dict(responses.set_index('reply')['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = preprocess_data.create_dictionary(data_text, 1, 0.25, 95000) #our entire vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.loc[data['set'] == 'Train']\n",
    "df_train = df_train.reset_index()[['helpdesk_question', 'helpdesk_reply']]\n",
    "df_train_keep = df_train\n",
    "\n",
    "df_valid = data.loc[data['set'] == 'Valid']\n",
    "df_valid = df_valid.reset_index()[['helpdesk_question', 'helpdesk_reply']]\n",
    "\n",
    "df_test = data.loc[data['set'] == 'Test']\n",
    "df_test = df_test.reset_index()[['helpdesk_question', 'helpdesk_reply']]\n",
    "\n",
    "df_LR = data.loc[(data['set'] == 'Test') & (data['low_resource'] == 'True') ]\n",
    "df_LR = df_LR.reset_index()[['helpdesk_question', 'helpdesk_reply']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96412, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57545"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 30\n",
    "min_token_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = preprocess_data.create_lookup_tables(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the input sentence into a sequence of word IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96412, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x_word_ids = []\n",
    "for question in df_train['helpdesk_question'].apply(preprocess_data.preprocess_question, \n",
    "                                                    args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    train_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "train_x_word_ids = np.stack(train_x_word_ids)\n",
    "print(train_x_word_ids.shape)\n",
    "    \n",
    "val_x_word_ids = []\n",
    "for question in data['helpdesk_question'].loc[data['set'] == 'Valid'].apply(preprocess_data.preprocess_question, \n",
    "                                                                          args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    val_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "val_x_word_ids = np.stack(val_x_word_ids)\n",
    "\n",
    "test_x_word_ids = []\n",
    "for question in data['helpdesk_question'].loc[data['set'] == 'Test'].apply(preprocess_data.preprocess_question, \n",
    "                                                                          args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    test_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "    \n",
    "test_x_word_ids = np.stack(test_x_word_ids)\n",
    "\n",
    "LR_x_word_ids = []\n",
    "for question in data['helpdesk_question'].loc[(data['set'] == 'Test') & \n",
    "                                              (data['low_resource'] == 'True')].apply(preprocess_data.preprocess_question, \n",
    "                                                                          args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    LR_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "LR_x_word_ids = np.stack(LR_x_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(reply, all_responses):\n",
    "    \n",
    "    \"\"\" Constructs a one-hot vector for replies\n",
    "    \n",
    "    Args:\n",
    "        reply: query item \n",
    "        all_responses: dict containing all the template responses with their corresponding IDs\n",
    "    \n",
    "    Return:\n",
    "        a one-hot vector where the corresponding ID of the reply is the one-hot index\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Y = np.zeros(len(all_responses), dtype = int)\n",
    "    Y[all_responses[reply]] += 1\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_id(reply, all_responses):\n",
    "    \n",
    "    \"\"\" Returns integer ID corresponding to response for easy comparison and classification\n",
    "    \n",
    "    Args:\n",
    "        reply: query item \n",
    "        all_responses: dict containing all the template responses with their corresponding IDs\n",
    "        \n",
    "    Return: \n",
    "        integer corresponding to each response     \n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    return all_responses[reply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(list(df_train['helpdesk_reply'].apply(get_dummies, args = [responses])))\n",
    "valid_y = np.array(list(df_valid['helpdesk_reply'].apply(get_dummies, args = [responses])))\n",
    "test_y  = np.array(list(df_test['helpdesk_reply'].apply(get_dummies,  args = [responses])))\n",
    "LR_y    = np.array(list(df_LR['helpdesk_reply'].apply(get_dummies,    args = [responses])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_word_ids = train_x_word_ids.reshape(train_x_word_ids.shape[:-1])\n",
    "val_x_word_ids   = val_x_word_ids.reshape(val_x_word_ids.shape[:-1])\n",
    "test_x_word_ids  = test_x_word_ids.reshape(test_x_word_ids.shape[:-1])\n",
    "LR_x_word_ids    = LR_x_word_ids.reshape(LR_x_word_ids.shape[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove vectors where the input sentence yields a sequence of length 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zero_vectors = np.where(train_x_word_ids.sum(axis = 1) == 0.0)[0]\n",
    "for t in range(train_zero_vectors.shape[0]):\n",
    "    train_x_word_ids[train_zero_vectors[t]][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_zero_vectors = np.where(val_x_word_ids.sum(axis = 1) == 0.0)[0]\n",
    "for t in range(val_zero_vectors.shape[0]):\n",
    "    val_x_word_ids[val_zero_vectors[t]][0] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the encoder (from the Transformer)\n",
    "\n",
    "Original code obtained from https://www.tensorflow.org/tutorials/text/transformer with minor adaptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    \n",
    "    \"\"\" Multiplying angle rates and positions gives a map of the position encoding angles as a \n",
    "    function of depth. The angle rates range from 1 [rads/step] to min_rate [rads/step] over the \n",
    "    vector depth.\n",
    "    \n",
    "    Args:\n",
    "        pos: vector of positions\n",
    "        i: embedding vector\n",
    "        d_model: dimension of embedding vector\n",
    "        \n",
    "    Returns:\n",
    "        Vector of angle radians\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    angle_rate = 1/np.power(10000, ((2*i)/np.float32(d_model)))\n",
    "    return pos * angle_rate\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \n",
    "    \"\"\" Calculate positional encodings to inject information about relative and absolute positions/\n",
    "    The positional encodings are obtained by taking the sine and cosine of the angle radians.\n",
    "    \n",
    "    Args:\n",
    "        position: maximum position encoding\n",
    "        d_model: dimension of embedding vector\n",
    "    \n",
    "    Returns:\n",
    "        A positional encoding vector\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], \n",
    "                            np.arange(d_model)[np.newaxis, :], \n",
    "                            d_model)\n",
    "    \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \n",
    "    \"\"\" Calculate the attention weights. q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\" Multi-head attention consists of four parts: linear layers that split into heads, \n",
    "    scaled dot-product attention, the concatenation of heads, and a final linear layer.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \n",
    "        \"\"\" Split the last dimension into (num_heads, depth). \n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        Args:\n",
    "            x: feed forward layer\n",
    "            batch_size: number of items in a batch\n",
    "            \n",
    "        Returns:\n",
    "            tuple containing (batch size, number of heads, sequence length, depth)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        \n",
    "        \"\"\" Call function to split the heads of the linear layers. \n",
    "        Returns the scaled attention dense layer and attention weights\n",
    "        \n",
    "        Args:\n",
    "            q: query shape == (..., seq_len_q, depth)\n",
    "            k: key shape == (..., seq_len_k, depth)\n",
    "            v: value shape == (..., seq_len_v, depth_v)\n",
    "            mask: float tensor with shape broadcastable \n",
    "            \n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, \n",
    "                                                                              #seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    \n",
    "    \"\"\" Construct a two-layer feedforward NN with layer dimensions d_model and dff respectively \n",
    "    and ReLU activations between layers.\n",
    "    \n",
    "    Args:\n",
    "        d_model: dimension of embedding layer\n",
    "        dff: dimension of the second layer\n",
    "    \n",
    "    Returns:\n",
    "        A two-layer feedforward NN \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\" Each encoder layer consists of Multi-head attention (with padding mask) and pointwise \n",
    "    feedforward networks.\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        \n",
    "        \"\"\" Constructs the encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            x: sequential layer\n",
    "            training: flag indicating training or testing\n",
    "            mask: float tensor with shape broadcastable \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\" The Encoder consists of an input embedding, summed with positional encoding, and N encoder layers. \n",
    "    The summation is the input to the encoder layers. The output of the encoder is the input to the decoder.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers       \n",
    "        self.embedding = Embedding(input_vocab_size, d_model,)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask=None):\n",
    "        \n",
    "        \"\"\" This function constructs the encoder.\n",
    "        Note we move the dropout to right before the summation (of embedding and positional encodings).\n",
    "        \n",
    "        Args: \n",
    "            x: sequential layer\n",
    "            training: flag indicating training or testing\n",
    "            mask: float tensor with shape broadcastable \n",
    "            \n",
    "        Returns:\n",
    "            An encoder model \n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]        \n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x, training = training)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        #x = self.dropout(x, training = training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention_encoder(num_layers, max_features, input_length=30, model_dim=512, dff = 128, \n",
    "                                num_heads=4):\n",
    "    \n",
    "    \"\"\" Constructs a multihead attention encoder model\n",
    "    \n",
    "    Args:\n",
    "        num_layers: number of encoder layers\n",
    "        max_features: size of vocabulary\n",
    "        input_length: length of input sequence\n",
    "        model_dim: dimension of embedding vector\n",
    "        dff: dimension of second layer in pointwise FFNN\n",
    "        num_heads: number of heads to split\n",
    "    \n",
    "    Returns:\n",
    "        Model object\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = Input(shape=(input_length, ))\n",
    "    x = Encoder(num_layers, model_dim, num_heads, dff, max_features, maximum_position_encoding = 10000, \n",
    "                rate=0.5)(inputs)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(300, activation=None)(x)\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Attention Encoder with Average Pooling\n",
    "\n",
    "We use average pooling to construct a single feature vector from the variable-length sequence of encodings produced by the MHA Encoder. This is then connected to a single dense layer with 300 dimensions. Our MHA has 8 heads, 2 layers, and dropout of 50% to regularize the model during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_features = len(unique_words) + 1\n",
    "num_layers = 2\n",
    "\n",
    "model = multihead_attention_encoder(num_layers, max_features, input_length=30, model_dim=128,\n",
    "                                    num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, 30, 128)           7564928   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 300)               38700     \n",
      "=================================================================\n",
      "Total params: 7,603,628\n",
      "Trainable params: 7,603,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Triplet Loss Training\n",
    "\n",
    "We perform the Siamese triplet loss training with mini-batch sizes of 256, cosine as our distance function and a margin $m$ of 0.5. For online sampling we use a batch size of 256. Larger batch sizes consumed too much memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.triplet_semihard_loss(margin=0.5, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', verbose=1, restore_best_weights=False, patience=50)\n",
    "model.compile(loss=loss, optimizer=tf.keras.optimizers.Adadelta(learning_rate= 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96412 samples, validate on 31955 samples\n",
      "Epoch 1/1000\n",
      "96412/96412 [==============================] - 18s 187us/sample - loss: 0.4986 - val_loss: 0.4971\n",
      "Epoch 2/1000\n",
      "96412/96412 [==============================] - 15s 158us/sample - loss: 0.4945 - val_loss: 0.4923\n",
      "Epoch 3/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4921 - val_loss: 0.4893\n",
      "Epoch 4/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4909 - val_loss: 0.4882\n",
      "Epoch 5/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4896 - val_loss: 0.4873\n",
      "Epoch 6/1000\n",
      "96412/96412 [==============================] - 15s 160us/sample - loss: 0.4890 - val_loss: 0.4865\n",
      "Epoch 7/1000\n",
      "96412/96412 [==============================] - 16s 168us/sample - loss: 0.4887 - val_loss: 0.4858\n",
      "Epoch 8/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4878 - val_loss: 0.4857\n",
      "Epoch 9/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4875 - val_loss: 0.4853\n",
      "Epoch 10/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4867 - val_loss: 0.4833\n",
      "Epoch 11/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4863 - val_loss: 0.4825\n",
      "Epoch 12/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4859 - val_loss: 0.4839\n",
      "Epoch 13/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4853 - val_loss: 0.4830\n",
      "Epoch 14/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4843 - val_loss: 0.4816\n",
      "Epoch 15/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4841 - val_loss: 0.4825\n",
      "Epoch 16/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4837 - val_loss: 0.4802\n",
      "Epoch 17/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4829 - val_loss: 0.4777\n",
      "Epoch 18/1000\n",
      "96412/96412 [==============================] - 15s 160us/sample - loss: 0.4824 - val_loss: 0.4784\n",
      "Epoch 19/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4816 - val_loss: 0.4780\n",
      "Epoch 20/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4805 - val_loss: 0.4760\n",
      "Epoch 21/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4801 - val_loss: 0.4763\n",
      "Epoch 22/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4802 - val_loss: 0.4753\n",
      "Epoch 23/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4797 - val_loss: 0.4737\n",
      "Epoch 24/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4789 - val_loss: 0.4740\n",
      "Epoch 25/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4780 - val_loss: 0.4740\n",
      "Epoch 26/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4780 - val_loss: 0.4736\n",
      "Epoch 27/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4771 - val_loss: 0.4748\n",
      "Epoch 28/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4773 - val_loss: 0.4720\n",
      "Epoch 29/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4756 - val_loss: 0.4735\n",
      "Epoch 30/1000\n",
      "96412/96412 [==============================] - 15s 160us/sample - loss: 0.4760 - val_loss: 0.4728\n",
      "Epoch 31/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4754 - val_loss: 0.4750\n",
      "Epoch 32/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4748 - val_loss: 0.4713\n",
      "Epoch 33/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4742 - val_loss: 0.4716\n",
      "Epoch 34/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4750 - val_loss: 0.4691\n",
      "Epoch 35/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4739 - val_loss: 0.4707\n",
      "Epoch 36/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4737 - val_loss: 0.4700\n",
      "Epoch 37/1000\n",
      "96412/96412 [==============================] - 15s 160us/sample - loss: 0.4733 - val_loss: 0.4696\n",
      "Epoch 38/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4736 - val_loss: 0.4697\n",
      "Epoch 39/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4734 - val_loss: 0.4720\n",
      "Epoch 40/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4731 - val_loss: 0.4683\n",
      "Epoch 41/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4721 - val_loss: 0.4686\n",
      "Epoch 42/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4719 - val_loss: 0.4706\n",
      "Epoch 43/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4713 - val_loss: 0.4688\n",
      "Epoch 44/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4715 - val_loss: 0.4684\n",
      "Epoch 45/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4713 - val_loss: 0.4683\n",
      "Epoch 46/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4700 - val_loss: 0.4672\n",
      "Epoch 47/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4707 - val_loss: 0.4678\n",
      "Epoch 48/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4705 - val_loss: 0.4662\n",
      "Epoch 49/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4695 - val_loss: 0.4658\n",
      "Epoch 50/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4693 - val_loss: 0.4655\n",
      "Epoch 51/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4697 - val_loss: 0.4688\n",
      "Epoch 52/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4684 - val_loss: 0.4670\n",
      "Epoch 53/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4685 - val_loss: 0.4667\n",
      "Epoch 54/1000\n",
      "96412/96412 [==============================] - 16s 168us/sample - loss: 0.4696 - val_loss: 0.4662\n",
      "Epoch 55/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4682 - val_loss: 0.4668\n",
      "Epoch 56/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4672 - val_loss: 0.4662\n",
      "Epoch 57/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4682 - val_loss: 0.4693\n",
      "Epoch 58/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4672 - val_loss: 0.4658\n",
      "Epoch 59/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4671 - val_loss: 0.4637\n",
      "Epoch 60/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4678 - val_loss: 0.4646\n",
      "Epoch 61/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4668 - val_loss: 0.4668\n",
      "Epoch 62/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4666 - val_loss: 0.4652\n",
      "Epoch 63/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4667 - val_loss: 0.4662\n",
      "Epoch 64/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4655 - val_loss: 0.4643\n",
      "Epoch 65/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4653 - val_loss: 0.4656\n",
      "Epoch 66/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4647 - val_loss: 0.4638\n",
      "Epoch 67/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4644 - val_loss: 0.4655\n",
      "Epoch 68/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4649 - val_loss: 0.4648\n",
      "Epoch 69/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4650 - val_loss: 0.4656\n",
      "Epoch 70/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4646 - val_loss: 0.4675\n",
      "Epoch 71/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4634 - val_loss: 0.4641\n",
      "Epoch 72/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4636 - val_loss: 0.4626\n",
      "Epoch 73/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4639 - val_loss: 0.4647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4640 - val_loss: 0.4664\n",
      "Epoch 75/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4626 - val_loss: 0.4638\n",
      "Epoch 76/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4639 - val_loss: 0.4648\n",
      "Epoch 77/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4633 - val_loss: 0.4634\n",
      "Epoch 78/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4632 - val_loss: 0.4617\n",
      "Epoch 79/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4628 - val_loss: 0.4628\n",
      "Epoch 80/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4632 - val_loss: 0.4627\n",
      "Epoch 81/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4632 - val_loss: 0.4606\n",
      "Epoch 82/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4624 - val_loss: 0.4621\n",
      "Epoch 83/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4623 - val_loss: 0.4624\n",
      "Epoch 84/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4614 - val_loss: 0.4629\n",
      "Epoch 85/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4607 - val_loss: 0.4630\n",
      "Epoch 86/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4625 - val_loss: 0.4613\n",
      "Epoch 87/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4613 - val_loss: 0.4644\n",
      "Epoch 88/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4601 - val_loss: 0.4637\n",
      "Epoch 89/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4610 - val_loss: 0.4632\n",
      "Epoch 90/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4605 - val_loss: 0.4616\n",
      "Epoch 91/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4605 - val_loss: 0.4606\n",
      "Epoch 92/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4590 - val_loss: 0.4628\n",
      "Epoch 93/1000\n",
      "96412/96412 [==============================] - 16s 169us/sample - loss: 0.4605 - val_loss: 0.4609\n",
      "Epoch 94/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4587 - val_loss: 0.4624\n",
      "Epoch 95/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4593 - val_loss: 0.4637\n",
      "Epoch 96/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4592 - val_loss: 0.4634\n",
      "Epoch 97/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4584 - val_loss: 0.4640\n",
      "Epoch 98/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4593 - val_loss: 0.4595\n",
      "Epoch 99/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4576 - val_loss: 0.4632\n",
      "Epoch 100/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4581 - val_loss: 0.4638\n",
      "Epoch 101/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4584 - val_loss: 0.4625\n",
      "Epoch 102/1000\n",
      "96412/96412 [==============================] - 16s 168us/sample - loss: 0.4582 - val_loss: 0.4622\n",
      "Epoch 103/1000\n",
      "96412/96412 [==============================] - 16s 168us/sample - loss: 0.4575 - val_loss: 0.4605\n",
      "Epoch 104/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4567 - val_loss: 0.4625\n",
      "Epoch 105/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4577 - val_loss: 0.4655\n",
      "Epoch 106/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4572 - val_loss: 0.4602\n",
      "Epoch 107/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4564 - val_loss: 0.4634\n",
      "Epoch 108/1000\n",
      "96412/96412 [==============================] - 16s 168us/sample - loss: 0.4573 - val_loss: 0.4571\n",
      "Epoch 109/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4569 - val_loss: 0.4625\n",
      "Epoch 110/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4569 - val_loss: 0.4640\n",
      "Epoch 111/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4568 - val_loss: 0.4613\n",
      "Epoch 112/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4561 - val_loss: 0.4603\n",
      "Epoch 113/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4563 - val_loss: 0.4606\n",
      "Epoch 114/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4567 - val_loss: 0.4633\n",
      "Epoch 115/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4559 - val_loss: 0.4615\n",
      "Epoch 116/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4559 - val_loss: 0.4615\n",
      "Epoch 117/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4551 - val_loss: 0.4619\n",
      "Epoch 118/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4555 - val_loss: 0.4609\n",
      "Epoch 119/1000\n",
      "96412/96412 [==============================] - 16s 168us/sample - loss: 0.4560 - val_loss: 0.4570\n",
      "Epoch 120/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4546 - val_loss: 0.4603\n",
      "Epoch 121/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4545 - val_loss: 0.4589\n",
      "Epoch 122/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4549 - val_loss: 0.4620\n",
      "Epoch 123/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4539 - val_loss: 0.4590\n",
      "Epoch 124/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4540 - val_loss: 0.4592\n",
      "Epoch 125/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4543 - val_loss: 0.4619\n",
      "Epoch 126/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4532 - val_loss: 0.4622\n",
      "Epoch 127/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4550 - val_loss: 0.4584\n",
      "Epoch 128/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4529 - val_loss: 0.4600\n",
      "Epoch 129/1000\n",
      "96412/96412 [==============================] - 15s 161us/sample - loss: 0.4543 - val_loss: 0.4629\n",
      "Epoch 130/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4541 - val_loss: 0.4626\n",
      "Epoch 131/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4533 - val_loss: 0.4606\n",
      "Epoch 132/1000\n",
      "96412/96412 [==============================] - 15s 160us/sample - loss: 0.4532 - val_loss: 0.4594\n",
      "Epoch 133/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4539 - val_loss: 0.4594\n",
      "Epoch 134/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4542 - val_loss: 0.4610\n",
      "Epoch 135/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4537 - val_loss: 0.4591\n",
      "Epoch 136/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4522 - val_loss: 0.4577\n",
      "Epoch 137/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4530 - val_loss: 0.4629\n",
      "Epoch 138/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4520 - val_loss: 0.4625\n",
      "Epoch 139/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4524 - val_loss: 0.4603\n",
      "Epoch 140/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4522 - val_loss: 0.4594\n",
      "Epoch 141/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4504 - val_loss: 0.4608\n",
      "Epoch 142/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4512 - val_loss: 0.4595\n",
      "Epoch 143/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4503 - val_loss: 0.4599\n",
      "Epoch 144/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4503 - val_loss: 0.4593\n",
      "Epoch 145/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4512 - val_loss: 0.4595\n",
      "Epoch 146/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96412/96412 [==============================] - 16s 168us/sample - loss: 0.4504 - val_loss: 0.4584\n",
      "Epoch 147/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4507 - val_loss: 0.4616\n",
      "Epoch 148/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4512 - val_loss: 0.4596\n",
      "Epoch 149/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4504 - val_loss: 0.4613\n",
      "Epoch 150/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4502 - val_loss: 0.4594\n",
      "Epoch 151/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4506 - val_loss: 0.4604\n",
      "Epoch 152/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4492 - val_loss: 0.4602\n",
      "Epoch 153/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4500 - val_loss: 0.4597\n",
      "Epoch 154/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4499 - val_loss: 0.4605\n",
      "Epoch 155/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4496 - val_loss: 0.4618\n",
      "Epoch 156/1000\n",
      "96412/96412 [==============================] - 16s 168us/sample - loss: 0.4499 - val_loss: 0.4612\n",
      "Epoch 157/1000\n",
      "96412/96412 [==============================] - 15s 160us/sample - loss: 0.4495 - val_loss: 0.4621\n",
      "Epoch 158/1000\n",
      "96412/96412 [==============================] - 16s 162us/sample - loss: 0.4491 - val_loss: 0.4609\n",
      "Epoch 159/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4486 - val_loss: 0.4594\n",
      "Epoch 160/1000\n",
      "96412/96412 [==============================] - 16s 165us/sample - loss: 0.4479 - val_loss: 0.4604\n",
      "Epoch 161/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4487 - val_loss: 0.4611\n",
      "Epoch 162/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4476 - val_loss: 0.4631\n",
      "Epoch 163/1000\n",
      "96412/96412 [==============================] - 15s 161us/sample - loss: 0.4465 - val_loss: 0.4583\n",
      "Epoch 164/1000\n",
      "96412/96412 [==============================] - 16s 163us/sample - loss: 0.4479 - val_loss: 0.4602\n",
      "Epoch 165/1000\n",
      "96412/96412 [==============================] - 16s 167us/sample - loss: 0.4480 - val_loss: 0.4605\n",
      "Epoch 166/1000\n",
      "96412/96412 [==============================] - 16s 161us/sample - loss: 0.4477 - val_loss: 0.4626\n",
      "Epoch 167/1000\n",
      "96412/96412 [==============================] - 16s 166us/sample - loss: 0.4472 - val_loss: 0.4633\n",
      "Epoch 168/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4471 - val_loss: 0.4618\n",
      "Epoch 169/1000\n",
      "96412/96412 [==============================] - 16s 164us/sample - loss: 0.4458 - val_loss: 0.4609\n",
      "Epoch 00169: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5113f55b38>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x_word_ids, np.array(df_train['helpdesk_reply'].apply(get_label_id, args = [responses])),\n",
    "          batch_size=256,\n",
    "          epochs=1000,         \n",
    "          callbacks=[es],\n",
    "          validation_data=(val_x_word_ids, np.array(df_valid['helpdesk_reply'].apply(get_label_id, \n",
    "                                                                                     args = [responses]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_preprocess(entry):\n",
    "        \n",
    "    \"\"\" Returns integer ID corresponding to response for easy comparison and classification\n",
    "    \n",
    "    Args:\n",
    "        entry: query item \n",
    "        responses: dict containing all the template responses with their corresponding IDs\n",
    "        \n",
    "    Return: \n",
    "        integer corresponding to each response     \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if responses.get(entry) != None:\n",
    "        return responses[entry]\n",
    "    else:\n",
    "        return len(responses) #default unknown class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = model.predict(train_x_word_ids)\n",
    "y_train = df_train_keep['helpdesk_reply'].apply(label_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = model.predict(val_x_word_ids)\n",
    "y_valid = df_valid['helpdesk_reply'].apply(label_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = model.predict(test_x_word_ids)\n",
    "y_test = df_test['helpdesk_reply'].apply(label_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_LR = model.predict(LR_x_word_ids)\n",
    "y_LR = df_LR['helpdesk_reply'].apply(label_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn_model(x_train, y_train, metric, k, weights):\n",
    "            \n",
    "    \"\"\" Fit k-nearest neighbour model to the sentence embeddings\n",
    "    \n",
    "    Args:\n",
    "        x_train: matrix of sentence embeddings\n",
    "        y_train: class labels associated with each sentence embedding \n",
    "        metric: distance metric to use\n",
    "        k: number of neighbours to consider\n",
    "        weights: to either use uniform voting (equal weighting) or weighted voting (the weight of \n",
    "        each vote is proportional to its distance to query)\n",
    "        \n",
    "    Returns:\n",
    "        A trained KNN classifier\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(k, 'Nearest Neighbours')\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, weights= weights, metric = metric)\n",
    "    clf.fit(x_train, y_train)\n",
    "    #print(\"Train accuracy\", clf.score(x_train, y_train))\n",
    "        \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Nearest Neighbours\n",
      "Train accuracy 0.9678048375720865\n",
      "Validation accuracy 0.5283367235174464\n"
     ]
    }
   ],
   "source": [
    "clf_1NN = train_knn_model(x_train = x_train, y_train = y_train, metric = 'cosine', \n",
    "                          k = 1, weights = 'distance')\n",
    "score = clf_1NN.score(x_train, y_train)\n",
    "print(\"Train accuracy\", score)\n",
    "score = clf_1NN.score(x_valid, y_valid)\n",
    "print(\"Validation accuracy\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Nearest Neighbours\n",
      "Validation accuracy 0.5668596463777187\n"
     ]
    }
   ],
   "source": [
    "clf_5NN = train_knn_model(x_train = x_train, y_train = y_train, metric = 'cosine', \n",
    "                          k = 5, weights = 'distance')\n",
    "score = clf_5NN.score(x_valid, y_valid)\n",
    "print(\"Validation accuracy\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Nearest Neighbours\n",
      "Validation accuracy 0.5831951181348771\n"
     ]
    }
   ],
   "source": [
    "clf_25NN = train_knn_model(x_train = x_train, y_train = y_train, metric = 'cosine', \n",
    "                          k = 25, weights = 'distance')\n",
    "score = clf_25NN.score(x_valid, y_valid)\n",
    "print(\"Validation accuracy\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Nearest Neighbours\n",
      "Validation accuracy 0.5836332342356438\n"
     ]
    }
   ],
   "source": [
    "clf_50NN = train_knn_model(x_train = x_train, y_train = y_train, metric = 'cosine', \n",
    "                          k = 50, weights = 'distance')\n",
    "score = clf_50NN.score(x_valid, y_valid)\n",
    "print(\"Validation accuracy\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on 1-NN 0.5298607017652717\n",
      "Test accuracy on 5-NN 0.5683616169763906\n",
      "Test accuracy on 25-NN 0.586976080414482\n",
      "Test accuracy on 50-NN 0.5855799956566252\n"
     ]
    }
   ],
   "source": [
    "score = clf_1NN.score(x_test, y_test)\n",
    "print(\"Test accuracy on 1-NN\", score)\n",
    "score = clf_5NN.score(x_test, y_test)\n",
    "print(\"Test accuracy on 5-NN\", score)\n",
    "score = clf_25NN.score(x_test, y_test)\n",
    "print(\"Test accuracy on 25-NN\", score)\n",
    "score = clf_50NN.score(x_test, y_test)\n",
    "print(\"Test accuracy on 50-NN\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Test accuracy on 1-NN 0.42818509615384615\n",
      "LR Test accuracy on 5-NN 0.4586838942307692\n",
      "LR Test accuracy on 25-NN 0.47866586538461536\n",
      "LR Test accuracy on 50-NN 0.48091947115384615\n"
     ]
    }
   ],
   "source": [
    "score = clf_1NN.score(x_LR, y_LR)\n",
    "print(\"LR Test accuracy on 1-NN\", score)\n",
    "score = clf_5NN.score(x_LR, y_LR)\n",
    "print(\"LR Test accuracy on 5-NN\", score)\n",
    "score = clf_25NN.score(x_LR, y_LR)\n",
    "print(\"LR Test accuracy on 25-NN\", score)\n",
    "score = clf_50NN.score(x_LR, y_LR)\n",
    "print(\"LR Test accuracy on 50-NN\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the quality of cross-lingual embeddings\n",
    "\n",
    "We design a small experiment to assess the quality of the cross-lingual embeddings for English and Zulu. The translations were obtained using google translate and verified by a Zulu speaker. We compute the sentence embedding for each English-Zulu translation pair and calculate the cosine distance between the two embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_embeddings(question, model, unique_words, min_token_length, word_to_id):\n",
    "    \n",
    "    \"\"\"Create sentence embeddings from the output of the pretrained model\n",
    "\n",
    "    Args:\n",
    "        question: \n",
    "        model: pretrained sentence embedding model\n",
    "        unique_words:\n",
    "        min_token_length:\n",
    "        word_to_id:\n",
    "        \n",
    "    Returns:\n",
    "        A sentence embedding for the input question\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    q = preprocess_data.preprocess_question(question, unique_words, min_token_length)\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(q, word_to_id)\n",
    "    word_ids = np.array(word_ids, dtype = float)\n",
    "    word_ids = word_ids.reshape((1, word_ids.shape[0]))\n",
    "    embedding = model.predict(word_ids)\n",
    "    return embedding    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_A  = \"can you drink coca cola when you are pregnant\"\n",
    "zulu_A = \"ungayiphuza yini i-coca cola uma ukhulelwe\"\n",
    "\n",
    "eng_B  = \"when can i stop breastfeeding\"\n",
    "zulu_B = \"ngingakuyeka nini ukuncelisa ibele\"\n",
    "\n",
    "eng_C  = \"when can I start feeding my baby solid food\"\n",
    "zulu_C = \"ngingaqala nini ukondla ingane yami ukudla okuqinile\"\n",
    "\n",
    "eng_D  = \"what are the signs of labour\"\n",
    "zulu_D = \"yiziphi izimpawu zokubeletha\"\n",
    "\n",
    "eng_E  = \"when can I learn the gender of my baby\"\n",
    "zulu_E = \"ngingabazi ubulili bengane yami\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_eng_A = create_sentence_embeddings(eng_A, model, unique_words, min_token_length, word_to_id)\n",
    "embed_eng_B = create_sentence_embeddings(eng_B, model, unique_words, min_token_length, word_to_id)\n",
    "embed_eng_C = create_sentence_embeddings(eng_C, model, unique_words, min_token_length, word_to_id)\n",
    "embed_eng_D = create_sentence_embeddings(eng_D, model, unique_words, min_token_length, word_to_id)\n",
    "embed_eng_E = create_sentence_embeddings(eng_E, model, unique_words, min_token_length, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_zulu_A = create_sentence_embeddings(zulu_A, model, unique_words, min_token_length, word_to_id)\n",
    "embed_zulu_B = create_sentence_embeddings(zulu_B, model, unique_words, min_token_length, word_to_id)\n",
    "embed_zulu_C = create_sentence_embeddings(zulu_C, model, unique_words, min_token_length, word_to_id)\n",
    "embed_zulu_D = create_sentence_embeddings(zulu_D, model, unique_words, min_token_length, word_to_id)\n",
    "embed_zulu_E = create_sentence_embeddings(zulu_E, model, unique_words, min_token_length, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence A: 0.22015321254730225\n",
      "Sentence B: 0.3873268961906433\n",
      "Sentence C: 0.34619617462158203\n",
      "Sentence D: 0.3381393551826477\n",
      "Sentence E: 0.6258534491062164\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence A:\", cosine(embed_eng_A, embed_zulu_A))\n",
    "print(\"Sentence B:\", cosine(embed_eng_B, embed_zulu_B))\n",
    "print(\"Sentence C:\", cosine(embed_eng_C, embed_zulu_C))\n",
    "print(\"Sentence D:\", cosine(embed_eng_D, embed_zulu_D))\n",
    "print(\"Sentence E:\", cosine(embed_eng_E, embed_zulu_E))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
