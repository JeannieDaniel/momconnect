{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention with 128-dimensional embedding\n",
    "\n",
    "Author: Jeanne Elizabeth Daniel\n",
    "\n",
    "November 2019\n",
    "\n",
    "We employ the multi-head attention encoder of the Transformer (Vaswani et al., 2017), to model the multilingual questions. The Transformer is a new type of encoder-decoder model that relies solely on attention to draw global dependencies between the input and output sequences.\n",
    "\n",
    "Attention allows the model to focus on different parts of the input sequence at every step of the output sequence. This enables modelling dependencies without any regards for their distance in the sequences. This architecture is devoid of any recurrence or convolutions, and thus its training can be parallelizable.\n",
    "\n",
    "The encoder component is a stack of encoders, identical in structure, but all with their own set of weights. Each encoder consists of a self-attention layer, followed by a fully-connected feedforward layer. \n",
    "\n",
    "The attention used by the Transformer is the scaled dot-product attention with a set of queries in matrix $\\boldsymbol{Q}$, a set of keys in matrix $\\boldsymbol{K}$, and a set of values in matrix $\\boldsymbol{V}$, and is computed as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathrm{Attention} (\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) = \\mathrm{softmax} \\Bigg(\\frac{\\boldsymbol{Q} \\boldsymbol{K}^{\\top}}{ \\sqrt{d_K}}\\Bigg) \\boldsymbol{V},\n",
    "\\end{equation}\n",
    "\n",
    "where $d_K$ is the dimension of the keys and acts as a scaling factor. Multi-headed attention allows for attention to be aggregated across $h$ different, randomly-initialized representation subspaces. Thus,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathrm{MultiHead} (\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) = \\mathrm{Concat} (\\mathrm{head}_1, \\dots, \\mathrm{head}_h) \\boldsymbol{W}^{O},\n",
    "\\end{equation}\n",
    "\n",
    "where Concat refers to concatenating each head, defined as: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathrm{head}_i = \\mathrm{Attention} (\\boldsymbol{Q}\\boldsymbol{W}^{Q}_i, \\boldsymbol{K}\\boldsymbol{W}^{K}_i, \\boldsymbol{V}\\boldsymbol{W}^{V}_i ),\n",
    "\\end{equation}\n",
    "\n",
    "with $\\boldsymbol{W}^{Q}_i \\in \\mathbb{R}^{d_{\\mathrm{model}} \\times d_V }, \\boldsymbol{W}^K_i \\in \\mathbb{R}^{d_{\\mathrm{model}} \\times d_V }$, \n",
    "$\\boldsymbol{W}_i^V \\in \\mathbb{R}^{d_{\\mathrm{model}} \\times d_V}$,\n",
    "and $\\boldsymbol{W}^O \\in \\mathbb{R}^{h d_v \\times d_{\\mathrm{model}}}$. \n",
    "\n",
    "The scalar $d_V$ represents the dimension of the values and $d_{\\mathrm{model}}$ denotes the dimension of the model's embedding space. This multi-headed attention function can also be parallelized and trained across multiple computers. The authors also inject information about the relative and absolute positions of the values in the sequence using positional encoding to allow for the modelling of time-dependencies. \n",
    "\n",
    "This is done by summing the positional encodings with the input embeddings, which are defined as \n",
    "\\begin{eqnarray}\n",
    "PE_{(pos, 2i)}  &  = & \\sin (pos/10000^{2i/d_{\\mathrm{model}}}),\\\\\n",
    "PE_{(pos, 2i+1)} & = & \\cos (pos/10000^{2i/d_{\\mathrm{model}}}).\n",
    "\\end{eqnarray}\n",
    "Combining all these elements results in state-of-the-art embeddings that, when compared to previous models, has reduced computational complexity per layer, parallelizable computation, and better long-term dependency modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.append(os.path.join(\\\"..\\\")) # path to source relative to current directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess_data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, TimeDistributed, Input, Flatten, AdditiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_7B', delimiter = ';', engine = 'python')\n",
    "data_text = data.loc[data['set'] == 'Train'][['helpdesk_question']]\n",
    "number_of_classes = data.loc[data['set'] == 'Train']['helpdesk_reply'].value_counts().shape[0]\n",
    "data = data[['helpdesk_question', 'helpdesk_reply', 'set', 'low_resource']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = pd.DataFrame(data.loc[data['set'] == 'Train']['helpdesk_reply'].value_counts()).reset_index()\n",
    "responses['reply'] = responses['index']\n",
    "responses['index'] = responses.index\n",
    "responses = dict(responses.set_index('reply')['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = preprocess_data.create_dictionary(data_text, 1, 0.25, 95000) #our entire vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.loc[data['set'] == 'Train']\n",
    "df_train = df_train.reset_index()[['helpdesk_question', 'helpdesk_reply']]\n",
    "\n",
    "df_valid = data.loc[data['set'] == 'Valid']\n",
    "df_valid = df_valid.reset_index()[['helpdesk_question', 'helpdesk_reply']]\n",
    "\n",
    "df_test = data.loc[data['set'] == 'Test']\n",
    "df_test = df_test.reset_index()[['helpdesk_question', 'helpdesk_reply']]\n",
    "\n",
    "df_LR = data.loc[(data['set'] == 'Test') & (data['low_resource'] == 'True') ]\n",
    "df_LR = df_LR.reset_index()[['helpdesk_question', 'helpdesk_reply']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96412, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57545"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 30\n",
    "min_token_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = preprocess_data.create_lookup_tables(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the input sentence into a sequence of word IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96412, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x_word_ids = []\n",
    "for question in df_train['helpdesk_question'].apply(preprocess_data.preprocess_question, \n",
    "                                                    args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    train_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "train_x_word_ids = np.stack(train_x_word_ids)\n",
    "print(train_x_word_ids.shape)\n",
    "    \n",
    "val_x_word_ids = []\n",
    "for question in data['helpdesk_question'].loc[data['set'] == 'Valid'].apply(preprocess_data.preprocess_question, \n",
    "                                                                          args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    val_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "val_x_word_ids = np.stack(val_x_word_ids)\n",
    "\n",
    "test_x_word_ids = []\n",
    "for question in data['helpdesk_question'].loc[data['set'] == 'Test'].apply(preprocess_data.preprocess_question, \n",
    "                                                                          args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    test_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "    \n",
    "test_x_word_ids = np.stack(test_x_word_ids)\n",
    "\n",
    "LR_x_word_ids = []\n",
    "for question in data['helpdesk_question'].loc[(data['set'] == 'Test') & \n",
    "                                              (data['low_resource'] == 'True')].apply(preprocess_data.preprocess_question, \n",
    "                                                                          args = [unique_words, min_token_length]):\n",
    "    word_ids = preprocess_data.transform_sequence_to_word_ids(question, word_to_id)\n",
    "    LR_x_word_ids.append(np.array(word_ids, dtype = float))\n",
    "LR_x_word_ids = np.stack(LR_x_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(reply, all_responses):\n",
    "    \n",
    "    \"\"\" Constructs a one-hot vector for replies\n",
    "    \n",
    "    Args:\n",
    "        reply: query item \n",
    "        all_responses: dict containing all the template responses with their corresponding IDs\n",
    "    \n",
    "    Return:\n",
    "        a one-hot vector where the corresponding ID of the reply is the one-hot index\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Y = np.zeros(len(all_responses), dtype = int)\n",
    "    Y[all_responses[reply]] += 1\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(list(df_train['helpdesk_reply'].apply(get_dummies, args = [responses])))\n",
    "valid_y = np.array(list(df_valid['helpdesk_reply'].apply(get_dummies, args = [responses])))\n",
    "test_y  = np.array(list(df_test['helpdesk_reply'].apply(get_dummies,  args = [responses])))\n",
    "LR_y    = np.array(list(df_LR['helpdesk_reply'].apply(get_dummies,    args = [responses])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_word_ids = train_x_word_ids.reshape(train_x_word_ids.shape[:-1])\n",
    "val_x_word_ids   = val_x_word_ids.reshape(val_x_word_ids.shape[:-1])\n",
    "test_x_word_ids  = test_x_word_ids.reshape(test_x_word_ids.shape[:-1])\n",
    "LR_x_word_ids    = LR_x_word_ids.reshape(LR_x_word_ids.shape[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform vectors where the input sentence yields a sequence of length 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zero_vectors = np.where(train_x_word_ids.sum(axis = 1) == 0.0)[0]\n",
    "for t in range(train_zero_vectors.shape[0]):\n",
    "    train_x_word_ids[train_zero_vectors[t]][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_zero_vectors = np.where(val_x_word_ids.sum(axis = 1) == 0.0)[0]\n",
    "for t in range(val_zero_vectors.shape[0]):\n",
    "    val_x_word_ids[val_zero_vectors[t]][0] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the encoder (from the Transformer)\n",
    "\n",
    "Original code obtained from https://www.tensorflow.org/tutorials/text/transformer with minor adaptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    \n",
    "    \"\"\" Multiplying angle rates and positions gives a map of the position encoding angles as a \n",
    "    function of depth. The angle rates range from 1 [rads/step] to min_rate [rads/step] over the \n",
    "    vector depth.\n",
    "    \n",
    "    Args:\n",
    "        pos: vector of positions\n",
    "        i: embedding vector\n",
    "        d_model: dimension of embedding vector\n",
    "        \n",
    "    Returns:\n",
    "        Vector of angle radians\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    angle_rate = 1/np.power(10000, ((2*i)/np.float32(d_model)))\n",
    "    return pos * angle_rate\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \n",
    "    \"\"\" Calculate positional encodings to inject information about relative and absolute positions/\n",
    "    The positional encodings are obtained by taking the sine and cosine of the angle radians.\n",
    "    \n",
    "    Args:\n",
    "        position: maximum position encoding\n",
    "        d_model: dimension of embedding vector\n",
    "    \n",
    "    Returns:\n",
    "        A positional encoding vector\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], \n",
    "                            np.arange(d_model)[np.newaxis, :], \n",
    "                            d_model)\n",
    "    \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \n",
    "    \"\"\" Calculate the attention weights. q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\" Multi-head attention consists of four parts: linear layers that split into heads, \n",
    "    scaled dot-product attention, the concatenation of heads, and a final linear layer.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \n",
    "        \"\"\" Split the last dimension into (num_heads, depth). \n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        Args:\n",
    "            x: feed forward layer\n",
    "            batch_size: number of items in a batch\n",
    "            \n",
    "        Returns:\n",
    "            tuple containing (batch size, number of heads, sequence length, depth)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        \n",
    "        \"\"\" Call function to split the heads of the linear layers. \n",
    "        Returns the scaled attention dense layer and attention weights\n",
    "        \n",
    "        Args:\n",
    "            q: query shape == (..., seq_len_q, depth)\n",
    "            k: key shape == (..., seq_len_k, depth)\n",
    "            v: value shape == (..., seq_len_v, depth_v)\n",
    "            mask: float tensor with shape broadcastable \n",
    "            \n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, \n",
    "                                                                              #seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    \n",
    "    \"\"\" Construct a two-layer feedforward NN with layer dimensions d_model and dff respectively \n",
    "    and ReLU activations between layers.\n",
    "    \n",
    "    Args:\n",
    "        d_model: dimension of embedding layer\n",
    "        dff: dimension of the second layer\n",
    "    \n",
    "    Returns:\n",
    "        A two-layer feedforward NN \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\" Each encoder layer consists of Multi-head attention (with padding mask) and pointwise \n",
    "    feedforward networks.\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        \n",
    "        \"\"\" Constructs the encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            x: sequential layer\n",
    "            training: flag indicating training or testing\n",
    "            mask: float tensor with shape broadcastable \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\" The Encoder consists of an input embedding, summed with positional encoding, and N encoder layers. \n",
    "    The summation is the input to the encoder layers. The output of the encoder is the input to the decoder.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers       \n",
    "        self.embedding = Embedding(input_vocab_size, d_model,)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask=None):\n",
    "        \n",
    "        \"\"\" This function constructs the encoder.\n",
    "        Note we move the dropout to right before the summation (of embedding and positional encodings).\n",
    "        \n",
    "        Args: \n",
    "            x: sequential layer\n",
    "            training: flag indicating training or testing\n",
    "            mask: float tensor with shape broadcastable \n",
    "            \n",
    "        Returns:\n",
    "            An encoder model \n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]        \n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x, training = training)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        #x = self.dropout(x, training = training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention_encoder(num_layers, max_features, input_length=30, model_dim=512, dff = 128, \n",
    "                                num_heads=4):\n",
    "    \n",
    "    \"\"\" Constructs a multihead attention encoder model\n",
    "    \n",
    "    Args:\n",
    "        num_layers: number of encoder layers\n",
    "        max_features: size of vocabulary\n",
    "        input_length: length of input sequence\n",
    "        model_dim: dimension of embedding vector\n",
    "        dff: dimension of second layer in pointwise FFNN\n",
    "        num_heads: number of heads to split\n",
    "    \n",
    "    Returns:\n",
    "        Model object\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = Input(shape=(input_length, ))\n",
    "    x = Encoder(num_layers, model_dim, num_heads, dff, max_features, maximum_position_encoding = 10000, \n",
    "                rate=0.5)(inputs)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(89, activation='softmax')(x)\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Attention Encoder with Average Pooling\n",
    "\n",
    "We use average pooling to construct a single feature vector from the variable-length sequence of encodings produced by the MHA Encoder. This is then connected to a classification layer. Our MHA has 8 heads, 2 layers, and dropout of 50% to regularize the model during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_features = len(unique_words) + 1\n",
    "num_layers = 2\n",
    "\n",
    "model = multihead_attention_encoder(num_layers, max_features, input_length=30, model_dim=128,\n",
    "                                    num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "encoder_2 (Encoder)          (None, 30, 128)           7564928   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 89)                11481     \n",
      "=================================================================\n",
      "Total params: 7,576,409\n",
      "Trainable params: 7,576,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', verbose=1, restore_best_weights=True, patience=10)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.25, rho=0.95),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96412 samples, validate on 31955 samples\n",
      "Epoch 1/500\n",
      "96412/96412 [==============================] - 46s 476us/sample - loss: 2.4747 - accuracy: 0.4286 - val_loss: 2.0017 - val_accuracy: 0.4982\n",
      "Epoch 2/500\n",
      "96412/96412 [==============================] - 43s 451us/sample - loss: 1.9675 - accuracy: 0.4985 - val_loss: 1.7887 - val_accuracy: 0.5260\n",
      "Epoch 3/500\n",
      "96412/96412 [==============================] - 44s 451us/sample - loss: 1.8047 - accuracy: 0.5231 - val_loss: 1.7178 - val_accuracy: 0.5363\n",
      "Epoch 4/500\n",
      "96412/96412 [==============================] - 43s 441us/sample - loss: 1.7042 - accuracy: 0.5407 - val_loss: 1.6307 - val_accuracy: 0.5532\n",
      "Epoch 5/500\n",
      "96412/96412 [==============================] - 43s 447us/sample - loss: 1.6338 - accuracy: 0.5518 - val_loss: 1.6011 - val_accuracy: 0.5592\n",
      "Epoch 6/500\n",
      "96412/96412 [==============================] - 43s 448us/sample - loss: 1.5734 - accuracy: 0.5635 - val_loss: 1.5609 - val_accuracy: 0.5613\n",
      "Epoch 7/500\n",
      "96412/96412 [==============================] - 41s 422us/sample - loss: 1.5252 - accuracy: 0.5720 - val_loss: 1.5452 - val_accuracy: 0.5713\n",
      "Epoch 8/500\n",
      "96412/96412 [==============================] - 55s 570us/sample - loss: 1.4811 - accuracy: 0.5796 - val_loss: 1.5094 - val_accuracy: 0.5777\n",
      "Epoch 9/500\n",
      "96412/96412 [==============================] - 87s 905us/sample - loss: 1.4374 - accuracy: 0.5898 - val_loss: 1.5027 - val_accuracy: 0.5785\n",
      "Epoch 10/500\n",
      "96412/96412 [==============================] - 88s 909us/sample - loss: 1.4011 - accuracy: 0.5968 - val_loss: 1.4950 - val_accuracy: 0.5817\n",
      "Epoch 11/500\n",
      "96412/96412 [==============================] - 88s 918us/sample - loss: 1.3708 - accuracy: 0.6023 - val_loss: 1.4671 - val_accuracy: 0.5890\n",
      "Epoch 12/500\n",
      "96412/96412 [==============================] - 88s 911us/sample - loss: 1.3376 - accuracy: 0.6083 - val_loss: 1.4500 - val_accuracy: 0.5906\n",
      "Epoch 13/500\n",
      "96412/96412 [==============================] - 86s 888us/sample - loss: 1.3100 - accuracy: 0.6168 - val_loss: 1.4603 - val_accuracy: 0.5889\n",
      "Epoch 14/500\n",
      "96412/96412 [==============================] - 88s 911us/sample - loss: 1.2797 - accuracy: 0.6244 - val_loss: 1.4559 - val_accuracy: 0.5868\n",
      "Epoch 15/500\n",
      "96412/96412 [==============================] - 87s 902us/sample - loss: 1.2524 - accuracy: 0.6294 - val_loss: 1.4514 - val_accuracy: 0.5940\n",
      "Epoch 16/500\n",
      "96412/96412 [==============================] - 88s 909us/sample - loss: 1.2294 - accuracy: 0.6359 - val_loss: 1.4510 - val_accuracy: 0.5923\n",
      "Epoch 17/500\n",
      "96412/96412 [==============================] - 85s 879us/sample - loss: 1.2047 - accuracy: 0.6406 - val_loss: 1.4343 - val_accuracy: 0.6013\n",
      "Epoch 18/500\n",
      "96412/96412 [==============================] - 51s 533us/sample - loss: 1.1790 - accuracy: 0.6459 - val_loss: 1.4554 - val_accuracy: 0.5893\n",
      "Epoch 19/500\n",
      "96412/96412 [==============================] - 48s 499us/sample - loss: 1.1525 - accuracy: 0.6530 - val_loss: 1.4351 - val_accuracy: 0.5981\n",
      "Epoch 20/500\n",
      "96412/96412 [==============================] - 50s 517us/sample - loss: 1.1365 - accuracy: 0.6570 - val_loss: 1.4549 - val_accuracy: 0.5908\n",
      "Epoch 21/500\n",
      "96412/96412 [==============================] - 49s 510us/sample - loss: 1.1176 - accuracy: 0.6609 - val_loss: 1.4692 - val_accuracy: 0.5914\n",
      "Epoch 22/500\n",
      "96412/96412 [==============================] - 49s 510us/sample - loss: 1.0961 - accuracy: 0.6679 - val_loss: 1.4551 - val_accuracy: 0.5984\n",
      "Epoch 23/500\n",
      "96412/96412 [==============================] - 50s 515us/sample - loss: 1.0781 - accuracy: 0.6708 - val_loss: 1.4614 - val_accuracy: 0.5988\n",
      "Epoch 24/500\n",
      "96412/96412 [==============================] - 48s 496us/sample - loss: 1.0573 - accuracy: 0.6775 - val_loss: 1.4784 - val_accuracy: 0.5967\n",
      "Epoch 25/500\n",
      "96412/96412 [==============================] - 51s 528us/sample - loss: 1.0401 - accuracy: 0.6815 - val_loss: 1.4696 - val_accuracy: 0.6021\n",
      "Epoch 26/500\n",
      "96412/96412 [==============================] - 50s 522us/sample - loss: 1.0269 - accuracy: 0.6858 - val_loss: 1.4849 - val_accuracy: 0.6013\n",
      "Epoch 27/500\n",
      "96412/96412 [==============================] - 52s 540us/sample - loss: 1.0066 - accuracy: 0.6911 - val_loss: 1.4972 - val_accuracy: 0.6013\n",
      "Epoch 28/500\n",
      "96412/96412 [==============================] - 48s 502us/sample - loss: 0.9951 - accuracy: 0.6942 - val_loss: 1.4832 - val_accuracy: 0.6049\n",
      "Epoch 29/500\n",
      "96412/96412 [==============================] - 50s 516us/sample - loss: 0.9797 - accuracy: 0.6979 - val_loss: 1.5083 - val_accuracy: 0.5992\n",
      "Epoch 30/500\n",
      "96412/96412 [==============================] - 51s 525us/sample - loss: 0.9598 - accuracy: 0.7031 - val_loss: 1.5074 - val_accuracy: 0.6021\n",
      "Epoch 31/500\n",
      "96412/96412 [==============================] - 49s 508us/sample - loss: 0.9468 - accuracy: 0.7064 - val_loss: 1.5012 - val_accuracy: 0.6059\n",
      "Epoch 32/500\n",
      "96412/96412 [==============================] - 50s 514us/sample - loss: 0.9336 - accuracy: 0.7115 - val_loss: 1.5113 - val_accuracy: 0.6087\n",
      "Epoch 33/500\n",
      "96412/96412 [==============================] - 49s 504us/sample - loss: 0.9205 - accuracy: 0.7142 - val_loss: 1.5321 - val_accuracy: 0.6074\n",
      "Epoch 34/500\n",
      "96412/96412 [==============================] - 49s 508us/sample - loss: 0.9081 - accuracy: 0.7198 - val_loss: 1.5411 - val_accuracy: 0.6099\n",
      "Epoch 35/500\n",
      "96412/96412 [==============================] - 51s 525us/sample - loss: 0.8947 - accuracy: 0.7221 - val_loss: 1.5484 - val_accuracy: 0.6086\n",
      "Epoch 36/500\n",
      "96412/96412 [==============================] - 49s 508us/sample - loss: 0.8834 - accuracy: 0.7255 - val_loss: 1.5246 - val_accuracy: 0.6100\n",
      "Epoch 37/500\n",
      "96412/96412 [==============================] - 46s 479us/sample - loss: 0.8712 - accuracy: 0.7278 - val_loss: 1.5693 - val_accuracy: 0.6019\n",
      "Epoch 38/500\n",
      "96412/96412 [==============================] - 50s 517us/sample - loss: 0.8600 - accuracy: 0.7325 - val_loss: 1.5596 - val_accuracy: 0.6109\n",
      "Epoch 39/500\n",
      "96412/96412 [==============================] - 50s 518us/sample - loss: 0.8485 - accuracy: 0.7348 - val_loss: 1.5610 - val_accuracy: 0.6110\n",
      "Epoch 40/500\n",
      "96412/96412 [==============================] - 53s 546us/sample - loss: 0.8401 - accuracy: 0.7374 - val_loss: 1.5920 - val_accuracy: 0.6071\n",
      "Epoch 41/500\n",
      "96412/96412 [==============================] - 50s 514us/sample - loss: 0.8299 - accuracy: 0.7403 - val_loss: 1.6072 - val_accuracy: 0.6064\n",
      "Epoch 42/500\n",
      "96412/96412 [==============================] - 50s 517us/sample - loss: 0.8178 - accuracy: 0.7421 - val_loss: 1.6052 - val_accuracy: 0.6064\n",
      "Epoch 43/500\n",
      "96412/96412 [==============================] - 47s 488us/sample - loss: 0.8106 - accuracy: 0.7450 - val_loss: 1.6216 - val_accuracy: 0.6077\n",
      "Epoch 44/500\n",
      "96412/96412 [==============================] - 50s 522us/sample - loss: 0.8027 - accuracy: 0.7475 - val_loss: 1.6465 - val_accuracy: 0.6063\n",
      "Epoch 45/500\n",
      "96412/96412 [==============================] - 51s 530us/sample - loss: 0.7935 - accuracy: 0.7509 - val_loss: 1.6393 - val_accuracy: 0.6094\n",
      "Epoch 46/500\n",
      "96412/96412 [==============================] - 48s 502us/sample - loss: 0.7807 - accuracy: 0.7552 - val_loss: 1.6409 - val_accuracy: 0.6087\n",
      "Epoch 47/500\n",
      "96412/96412 [==============================] - 46s 477us/sample - loss: 0.7729 - accuracy: 0.7561 - val_loss: 1.6454 - val_accuracy: 0.6013\n",
      "Epoch 48/500\n",
      "96412/96412 [==============================] - 46s 478us/sample - loss: 0.7669 - accuracy: 0.7598 - val_loss: 1.6616 - val_accuracy: 0.6113\n",
      "Epoch 49/500\n",
      "96412/96412 [==============================] - 37s 388us/sample - loss: 0.7585 - accuracy: 0.7616 - val_loss: 1.6803 - val_accuracy: 0.6115\n",
      "Epoch 50/500\n",
      "96412/96412 [==============================] - 47s 491us/sample - loss: 0.7554 - accuracy: 0.7611 - val_loss: 1.6682 - val_accuracy: 0.6067\n",
      "Epoch 51/500\n",
      "96412/96412 [==============================] - 43s 450us/sample - loss: 0.7425 - accuracy: 0.7660 - val_loss: 1.6990 - val_accuracy: 0.6054\n",
      "Epoch 52/500\n",
      "96412/96412 [==============================] - 45s 467us/sample - loss: 0.7386 - accuracy: 0.7668 - val_loss: 1.7064 - val_accuracy: 0.6023\n",
      "Epoch 53/500\n",
      "96412/96412 [==============================] - 47s 484us/sample - loss: 0.7294 - accuracy: 0.7705 - val_loss: 1.7355 - val_accuracy: 0.6105\n",
      "Epoch 54/500\n",
      "96412/96412 [==============================] - 41s 430us/sample - loss: 0.7218 - accuracy: 0.7716 - val_loss: 1.7326 - val_accuracy: 0.6110\n",
      "Epoch 55/500\n",
      "96412/96412 [==============================] - 42s 437us/sample - loss: 0.7185 - accuracy: 0.7727 - val_loss: 1.7163 - val_accuracy: 0.6066\n",
      "Epoch 56/500\n",
      "96412/96412 [==============================] - 45s 470us/sample - loss: 0.7109 - accuracy: 0.7757 - val_loss: 1.7468 - val_accuracy: 0.6127\n",
      "Epoch 57/500\n",
      "96412/96412 [==============================] - 51s 527us/sample - loss: 0.7056 - accuracy: 0.7751 - val_loss: 1.7401 - val_accuracy: 0.6090\n",
      "Epoch 58/500\n",
      "96412/96412 [==============================] - 142s 1ms/sample - loss: 0.7014 - accuracy: 0.7780 - val_loss: 1.7405 - val_accuracy: 0.6054\n",
      "Epoch 59/500\n",
      "96412/96412 [==============================] - 172s 2ms/sample - loss: 0.6931 - accuracy: 0.7810 - val_loss: 1.7795 - val_accuracy: 0.6054\n",
      "Epoch 60/500\n",
      "96412/96412 [==============================] - 162s 2ms/sample - loss: 0.6897 - accuracy: 0.7825 - val_loss: 1.7376 - val_accuracy: 0.6042\n",
      "Epoch 61/500\n",
      "96412/96412 [==============================] - 161s 2ms/sample - loss: 0.6833 - accuracy: 0.7837 - val_loss: 1.7876 - val_accuracy: 0.6050\n",
      "Epoch 62/500\n",
      "96412/96412 [==============================] - 158s 2ms/sample - loss: 0.6792 - accuracy: 0.7847 - val_loss: 1.7986 - val_accuracy: 0.6091\n",
      "Epoch 63/500\n",
      "96412/96412 [==============================] - 159s 2ms/sample - loss: 0.6784 - accuracy: 0.7846 - val_loss: 1.8143 - val_accuracy: 0.6068\n",
      "Epoch 64/500\n",
      "96412/96412 [==============================] - 160s 2ms/sample - loss: 0.6724 - accuracy: 0.7866 - val_loss: 1.8504 - val_accuracy: 0.6121\n",
      "Epoch 65/500\n",
      "96412/96412 [==============================] - 154s 2ms/sample - loss: 0.6625 - accuracy: 0.7900 - val_loss: 1.8030 - val_accuracy: 0.6088\n",
      "Epoch 66/500\n",
      "96384/96412 [============================>.] - ETA: 0s - loss: 0.6604 - accuracy: 0.7907Restoring model weights from the end of the best epoch.\n",
      "96412/96412 [==============================] - 152s 2ms/sample - loss: 0.6603 - accuracy: 0.7907 - val_loss: 1.8117 - val_accuracy: 0.6076\n",
      "Epoch 00066: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4227c7bc50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x_word_ids, train_y, \n",
    "          batch_size=32,\n",
    "          epochs=500,\n",
    "          callbacks=[es],\n",
    "          validation_data=[val_x_word_ids, valid_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_score_top_1(word_ids, y_true, model):\n",
    "    \n",
    "    \"\"\" Computes classification accuracy for model.\n",
    "    \n",
    "    Args:\n",
    "        word_ids: matrix where each row is \n",
    "        y_true: ground truth labels\n",
    "        model: pretrained model\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    score = 0\n",
    "    probs = model.predict(word_ids)\n",
    "    for i in range(word_ids.shape[0]):\n",
    "        if y_true[i].argmax() == np.argsort(probs[i])[-1]:\n",
    "            score += 1\n",
    "        \n",
    "    print(\"Overall Accuracy:\", score/word_ids.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.6127053669222344\n"
     ]
    }
   ],
   "source": [
    "classifier_score_top_1(val_x_word_ids, valid_y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.617503800452952\n"
     ]
    }
   ],
   "source": [
    "classifier_score_top_1(test_x_word_ids, test_y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.5441706730769231\n"
     ]
    }
   ],
   "source": [
    "classifier_score_top_1(LR_x_word_ids, LR_y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-5 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_score_top_5(word_ids, y_true, model):\n",
    "    \n",
    "    \"\"\" Computes top-5 classification accuracy for model.\n",
    "    \n",
    "    Args:\n",
    "        word_ids: matrix where each row is \n",
    "        y_true: true labels\n",
    "        model: trained model\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    score = 0\n",
    "    probs = model.predict(word_ids)\n",
    "    for i in range(word_ids.shape[0]):\n",
    "        if y_true[i].argmax() in np.argsort(probs[i])[-5:]:\n",
    "            score += 1\n",
    "        \n",
    "    print(\"Overall Accuracy:\", score/word_ids.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9068966587038129\n"
     ]
    }
   ],
   "source": [
    "classifier_score_top_5(test_x_word_ids, test_y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8233173076923077\n"
     ]
    }
   ],
   "source": [
    "classifier_score_top_5(LR_x_word_ids, LR_y, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
