{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes + top 9000 bag-of-words\n",
    "Author: Jeanne Elizabeth Daniel\n",
    "\n",
    "Serves as baseline for MomConnect answer selection task.\n",
    "\n",
    "Note: we train and test in batches as the method is memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "     \"\"\" Method for preprocessing text\n",
    "    \n",
    "    Args:\n",
    "        text: string of text\n",
    "        min_token_length: integer value indicating min number of characters in a token\n",
    "        join: boolean indicating if function should join the list of tokens into the string or not\n",
    "    \n",
    "    Returns:\n",
    "        list of cleaned words\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(text) != str:\n",
    "        return []\n",
    "    \n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text, min_len =1, max_len = 40):\n",
    "        if len(token) > 2:\n",
    "            result.append(token)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_preprocess(entry):\n",
    "    \n",
    "    \"\"\" Returns integer ID corresponding to response for easy comparison and classification\n",
    "    \n",
    "    Args:\n",
    "        entry: query item \n",
    "        responses: dict containing all the template responses with their corresponding IDs\n",
    "        \n",
    "    Return: \n",
    "        integer corresponding to each response     \n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    if responses.get(entry) != None:\n",
    "        return responses[entry]\n",
    "    else:\n",
    "        return len(responses) #default unknown class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_feature_vector(text, features):\n",
    "    \n",
    "    \"\"\" Constructs a sparse feature vector containing list of word IDs\n",
    "    \n",
    "    Args:\n",
    "        text: text sentence\n",
    "        features: dict with words as keys and corresponding ids as values\n",
    "        \n",
    "    Returns:\n",
    "        list of word IDs for sequence of words\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sparse = []\n",
    "    \n",
    "    for n in preprocess(text):\n",
    "        f = features.get(n)\n",
    "        if f != None:\n",
    "            sparse.append(f)\n",
    "    return sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(df, features): \n",
    "    \n",
    "    \"\"\" Create batch of feature vectors in matrix form\n",
    "    \n",
    "    Args:\n",
    "        df: dataset of questions\n",
    "        features: vocabulary of words in dict form\n",
    "        \n",
    "    Returns:\n",
    "        matrix where rows are bag-of-words representation of questions\n",
    "        and columns represent the features (words)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    matrix = np.zeros((df.shape[0], len(features)))\n",
    "    all_text = list(df['helpdesk_question']) \n",
    "    \n",
    "    for i in range(len(all_text)):\n",
    "        sparse = create_sparse_feature_vector(all_text[i], features)\n",
    "        for s in sparse:\n",
    "            matrix[i][s] = 1\n",
    "            \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(df, features, increment= 10000):\n",
    "    \n",
    "    \"\"\" Train MNB in batches\n",
    "        \n",
    "    Args:\n",
    "        df: dataframe of questions\n",
    "        features: vocabulary of words in dict form\n",
    "        increment: size of batch\n",
    "        \n",
    "    Returns:\n",
    "        Trained MNB classifier\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Number of features:\", len(features))\n",
    "    clf = naive_bayes.MultinomialNB()\n",
    "\n",
    "    for i in range(0, df.shape[0], increment):\n",
    "        df_subset = df[i:i+increment]\n",
    "        train_batch = create_batch(df_subset, features)\n",
    "        clf.partial_fit(train_batch, \n",
    "                        df_subset['helpdesk_reply'].apply(label_preprocess), \n",
    "                        classes = list(range(len(responses) + 1)))\n",
    "        print(\"Train accuracy on batch\", i, \n",
    "              clf.score(train_batch, \n",
    "                        df_subset['helpdesk_reply'].apply(label_preprocess)))\n",
    "        \n",
    "    return clf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_validate(df, features, clf, increment= 10000):\n",
    "    \n",
    "    \"\"\" Test the accuracy of the MNB classifier \n",
    "    \n",
    "    Args:\n",
    "        df: dataframe of questions and responses\n",
    "        features: vocabulary of words in dict form\n",
    "        clf: trained classifier\n",
    "        increment: batch size\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    sum_of_scores = 0\n",
    "    for i in range(0, df.shape[0], increment):\n",
    "        df_subset = df[i:i+increment]\n",
    "        valid_batch = create_batch(df_subset, features)\n",
    "        score = clf.score(valid_batch, \n",
    "                          df_subset['helpdesk_reply'].apply(label_preprocess))\n",
    "        print(\"Validation Accuracy on batch:\", score)\n",
    "        sum_of_scores += int(score*df_subset.shape[0])\n",
    "        del valid_batch\n",
    "        \n",
    "    print(\"Overall Accuracy:\", sum_of_scores/df.shape[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_validate_top_5(df, features, clf, increment= 10000):\n",
    "    \n",
    "    \"\"\" Test the top-5 accuracy of the MNB classifier \n",
    "    \n",
    "    Args:\n",
    "        df: dataframe of questions and responses\n",
    "        features: vocabulary of words in dict form\n",
    "        clf: trained classifier\n",
    "        increment: batch size\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    sum_of_scores = 0\n",
    "    for i in range(0, df.shape[0], increment):\n",
    "        df_subset = df[i:i+increment]\n",
    "        valid_batch = create_batch(df_subset, features)\n",
    "        valid_responses = list(df_subset['helpdesk_reply'].apply(label_preprocess))\n",
    "        score = 0\n",
    "        for i in range(len(valid_batch)):\n",
    "            if valid_responses[i] in np.argsort(clf.predict_proba(valid_batch[i].reshape(1, -1)))[0][-5:]:\n",
    "                score += 1\n",
    "        \n",
    "        #score = clf.score(valid_batch, df_subset['helpdesk_reply'].apply(label_preprocess))\n",
    "        print(\"Validation Accuracy on batch:\", score/len(valid_batch))\n",
    "        sum_of_scores += score\n",
    "        del valid_batch, valid_responses\n",
    "        \n",
    "    print(\"Overall Accuracy:\", sum_of_scores/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_7B', delimiter = ';', engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_text = data.loc[data['set'] == 'Train'][['helpdesk_question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = data.loc[data['set'] == \n",
    "                             'Train']['helpdesk_reply'].value_counts().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = pd.DataFrame(data.loc[data['set'] == \n",
    "                            'Train']['helpdesk_reply'].value_counts()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_responses = dict(data['helpdesk_reply'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses['reply'] = responses['index']\n",
    "responses['index'] = responses.index\n",
    "responses = dict(responses.set_index('reply')['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data['helpdesk_question'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9286\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.3, keep_n=950000)\n",
    "print(len(dict(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_of_words = pd.DataFrame(pd.Series(dict(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_of_words['index'] = dictionary_of_words.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_of_words = dictionary_of_words.set_index(0)['index'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Naive Bayes classifier to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 9286\n",
      "Train accuracy on batch 0 0.585673982491806\n"
     ]
    }
   ],
   "source": [
    "clf = batch_train(data.loc[data['set'] == 'Train'], dictionary_of_words, increment = data.loc[data['set'] == 'Train'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve a training accuracy of 58.57%\n",
    "\n",
    "\n",
    "### Here we validate the model on the Full and Low-resource Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on batch: 0.5205\n",
      "Validation Accuracy on batch: 0.521\n",
      "Validation Accuracy on batch: 0.5188\n",
      "Validation Accuracy on batch: 0.5207161125319693\n",
      "Overall Accuracy: 0.5201063996244719\n"
     ]
    }
   ],
   "source": [
    "classifier_validate(data.loc[(data['set'] == 'Valid') ], dictionary_of_words, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on batch: 0.4380952380952381\n",
      "Overall Accuracy: 0.4380952380952381\n"
     ]
    }
   ],
   "source": [
    "classifier_validate(data.loc[(data['set'] == 'Valid') & (data['low_resource'] == 'True')], dictionary_of_words, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing\n",
    "\n",
    "We test the top-5 accuracy on the full test set and low-resource test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on batch: 0.8234\n",
      "Validation Accuracy on batch: 0.8241\n",
      "Validation Accuracy on batch: 0.819\n",
      "Validation Accuracy on batch: 0.8298253470667264\n",
      "Overall Accuracy: 0.8226972357521795\n"
     ]
    }
   ],
   "source": [
    "classifier_validate_top_5(data.loc[(data['set'] == 'Test') ], dictionary_of_words, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on batch: 0.7451923076923077\n",
      "Overall Accuracy: 0.7451923076923077\n"
     ]
    }
   ],
   "source": [
    "classifier_validate_top_5(data.loc[(data['set'] == 'Test') & (data['low_resource'] == 'True')], dictionary_of_words, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the top-1 accuracy on the full test set and low-resource test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on batch: 0.5235\n",
      "Validation Accuracy on batch: 0.5119\n",
      "Validation Accuracy on batch: 0.5269\n",
      "Validation Accuracy on batch: 0.5315718763994626\n",
      "Overall Accuracy: 0.5215152173238606\n"
     ]
    }
   ],
   "source": [
    "classifier_validate(data.loc[(data['set'] == 'Test') ], dictionary_of_words, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on batch: 0.43960336538461536\n",
      "Overall Accuracy: 0.43960336538461536\n"
     ]
    }
   ],
   "source": [
    "classifier_validate(data.loc[(data['set'] == 'Test') & (data['low_resource'] == 'True')], dictionary_of_words, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
